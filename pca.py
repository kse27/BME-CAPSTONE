# -*- coding: utf-8 -*-
"""BME Capstone

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c1vqEGlqlw-oNHchHQOYDC3zXh1Z6Gmw

# ğŸ§¸ BME Capstone
:predicting MDS-UPDR scores by using protein and peptide levels

ğŸ‘‰ 202100805 ê¹€ì‹œì€, 202102238 ìœ í¬ì§„

# ë°ì´í„° ì „ì²˜ë¦¬
"""

import csv
from collections import defaultdict
import pandas as pd

#Load a dataset into a Pandas DataFrame
train_proteins = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_proteins.csv")
train_peptides = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_peptides.csv")
train_clinical = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_clinical_data.csv")

#Uniprot, peptide ì¢…ë¥˜ ìˆ˜ í™•ì¸

unique_Uniprot = train_proteins['UniProt'].nunique()
print("Unique values in 'Uniprot' column:", unique_Uniprot)

unique_peptide=train_peptides['Peptide'].nunique()
print("Unique valeus in 'Peptide' column:", unique_peptide)

# Function to prepare dataset with all the steps mentioned above:
def prepare_dataset(train_proteins, train_peptides):
    # Step 1: Grouping
    df_protein_grouped = train_proteins.groupby(['visit_id','UniProt'])['NPX'].mean().reset_index()
    df_peptide_grouped = train_peptides.groupby(['visit_id','Peptide'])['PeptideAbundance'].mean().reset_index()

    # Step 2: Pivoting
    df_protein = df_protein_grouped.pivot(index='visit_id',columns = 'UniProt', values = 'NPX').rename_axis(columns=None).reset_index()
    df_peptide = df_peptide_grouped.pivot(index='visit_id',columns = 'Peptide', values = 'PeptideAbundance').rename_axis(columns=None).reset_index()

    # Step 3: Merging
    pro_pep_df = df_protein.merge(df_peptide, on = ['visit_id'], how = 'left')

    return pro_pep_df

pro_pep_df = prepare_dataset(train_proteins, train_peptides)
# CSV íŒŒì¼ë¡œ ì €ì¥
pro_pep_df.to_csv('/content/drive/MyDrive/BME/a_data.csv', index=False)
print(pro_pep_df.head())

import pandas as pd
a_data = pd.read_csv("/content/drive/MyDrive/BME/a_data.csv")
train_clinical = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_clinical_data.csv")

# visit_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‘ íŒŒì¼ ë³‘í•© (ì™¸ë¶€ ì¡°ì¸)
merged_df = pd.merge(a_data, train_clinical, on='visit_id', how='outer')

# ë§ˆì§€ë§‰ ì—´ì„ ì œê±°
df = merged_df.drop(merged_df.columns[-1], axis=1)

# 'patient id'ì™€ 'visitmonth' ì—´ì„ ì œê±°í•©ë‹ˆë‹¤
df = df.drop(['patient_id', 'visit_month'], axis=1)

# 'visit_id' ì—´ì„ '_'ë¡œ ë¶„í• í•˜ì—¬ 'patient_id'ì™€ 'visit_month'ë¡œ ë‚˜ëˆ„ê¸°
df[['patient_id', 'visit_month']] = df['visit_id'].str.split('_', expand=True)

# 'patient_id'ì™€ 'visit_month' ì—´ì˜ ë°ì´í„° íƒ€ì…ì„ ì ì ˆíˆ ë³€í™˜ (ì˜µì…˜)
df['patient_id'] = df['patient_id'].astype(int)
df['visit_month'] = df['visit_month'].astype(int)

# ì—´ ìˆœì„œ ì¡°ì •í•˜ì—¬ ë§¨ ë’¤ì— ìˆëŠ” ë‘ ê°œì˜ ì—´ì„ ë§¨ ì•ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
columns = df.columns.tolist()
new_order = columns[-2:] + columns[:-2]

# ìƒˆë¡œìš´ ìˆœì„œë¡œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
df = df[new_order]

# patient_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•œ í›„, visit_monthë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤
df_sorted = df.sort_values(by=['patient_id', 'visit_month'])

df_sorted

# CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤

from sklearn.impute import SimpleImputer

# ê° í™˜ìì˜ ê° ì—´ì— ëŒ€í•œ ì¡´ì¬í•˜ëŠ” ê°’ë“¤ì˜ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤
patient_means = df_sorted.groupby('patient_id').mean()

# null ê°’ì„ í•´ë‹¹ í™˜ìì˜ í‰ê·  ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤
imputer = SimpleImputer(strategy='mean')

feature_list = df_sorted.columns[3:-4]

# null ê°’ë§Œ ëŒ€ì²´í•©ë‹ˆë‹¤
for col in feature_list:
    mask = df_sorted[col].isnull()  # null ê°’ì„ í™•ì¸í•©ë‹ˆë‹¤
    if mask.any():  # null ê°’ì„ í¬í•¨í•˜ëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤
        for patient_id, mean_value in patient_means[col].items():
            df_sorted.loc[(df_sorted['patient_id'] == patient_id) & mask, col] = mean_value  # í•´ë‹¹ í™˜ìì˜ null ê°’ë§Œ ëŒ€ì²´í•©ë‹ˆë‹¤

df_sorted

#nullì„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê³ ë„ ë‚¨ì•„ìˆëŠ” nullê°’ì€ 0ìœ¼ë¡œ ëŒ€ì²´
for col in feature_list:# íŠ¹ì • ì—´ì— ìˆëŠ” null ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´ (ì˜ˆë¥¼ ë“¤ì–´, 'protein1' ì—´)
  df_sorted[col].fillna(0, inplace=True)

# ê²°ê³¼ í™•ì¸
print(df_sorted.isnull().sum())

df_sorted.to_csv('/content/drive/MyDrive/BME/modified_data2.csv', index=False)

"""#  ì‹œê³„ì—´ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì§€ ì•Šê³  ì˜ˆì¸¡

## XGBoost
"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

xgboost_dict = {}
mse_xg_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold ê°ì²´ ìƒì„±
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # ì¢…ì† ë³€ìˆ˜ ë° ë…ë¦½ ë³€ìˆ˜ ë¶„ë¦¬
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # ê° foldì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # XGBoost ëª¨ë¸ í•™ìŠµ
        xgboost = xgb.XGBRegressor()
        xgboost.fit(X_train, y_train)
        xgboost_pred = xgboost.predict(X_test)

        # ëª¨ë¸ ì €ì¥
        xgboost_dict[label]=xgboost

        # í‰ê°€: MSE ê³„ì‚°
        mse = mean_squared_error(y_test, xgboost_pred)
        mse_list.append(mse)

    # í‰ê·  MSE ê³„ì‚°
    avg_mse = sum(mse_list) / len(mse_list)
    mse_xg_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## RF"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor

rf_dict = {}
mse_rf_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold ê°ì²´ ìƒì„±
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # ì¢…ì† ë³€ìˆ˜ ë° ë…ë¦½ ë³€ìˆ˜ ë¶„ë¦¬
    X = data.drop(['visit_id','visit_month','patient_id','updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # ê° foldì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # RandomForest ëª¨ë¸ í•™ìŠµ
        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        rf.fit(X_train, y_train)
        rf_pred = rf.predict(X_test)

        # ëª¨ë¸ ì €ì¥
        rf_dict[label]= rf

        # í‰ê°€: MSE ê³„ì‚°
        mse = mean_squared_error(y_test, rf_pred)
        mse_list.append(mse)

    # í‰ê·  MSE ê³„ì‚°
    avg_mse = sum(mse_list) / len(mse_list)
    mse_rf_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## SVR"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR

svr_dict={}
mse_svr_dict={}

target = ['updrs_1','updrs_2','updrs_3','updrs_4']

# KFold ê°ì²´ ìƒì„±
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # ì¢…ì† ë³€ìˆ˜ ë° ë…ë¦½ ë³€ìˆ˜ ë¶„ë¦¬
    X = data.drop(['visit_id','visit_month','patient_id','updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # ê° foldì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # SVR ëª¨ë¸ í•™ìŠµ
        svr_rbf = SVR(kernel='rbf', C=100, gamma=0.5, epsilon=.1)
        svr_rbf.fit(X_train, y_train)
        svr_pred=svr_rbf.predict(X_test)

        # ëª¨ë¸ ì €ì¥
        svr_dict[label]=svr_rbf

        # í‰ê°€: MSE ê³„ì‚°
        mse = mean_squared_error(y_test, svr_pred)
        mse_list.append(mse)

    # í‰ê·  MSE ê³„ì‚°
    avg_mse = sum(mse_list) / len(mse_list)
    mse_svr_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## XGB+RF"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

# xgboost_dict = {}
# rf_dict = {}
m_dict={}
mse_1_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold ê°ì²´ ìƒì„±
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # ì¢…ì† ë³€ìˆ˜ ë° ë…ë¦½ ë³€ìˆ˜ ë¶„ë¦¬
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # ê° foldì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # XGBoost ëª¨ë¸ í•™ìŠµ
        xgboost_pred=xgboost_dict[label].predict(X_test)

        # RandomForest ëª¨ë¸ í•™ìŠµ
        rf_pred=rf_dict[label].predict(X_test)

        # ì•™ìƒë¸”ì„ ìœ„í•œ ì˜ˆì¸¡ê°’ ê²°í•©
        y_pred = (xgboost_pred + rf_pred) / 2

        # í‰ê°€: MSE ê³„ì‚°
        mse = mean_squared_error(y_test, y_pred)
        mse_list.append(mse)

    # í‰ê·  MSE ê³„ì‚°
    avg_mse = sum(mse_list) / len(mse_list)
    mse_1_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## XGB+SVR"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
import xgboost as xgb

# xgboost_dict = {}
# svr_dict = {}
mse_2_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold ê°ì²´ ìƒì„±
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # ì¢…ì† ë³€ìˆ˜ ë° ë…ë¦½ ë³€ìˆ˜ ë¶„ë¦¬
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # ê° foldì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # XGBoost ëª¨ë¸ í•™ìŠµ
        xgboost_pred=xgboost_dict[label].predict(X_test)

        # SVR ëª¨ë¸ í•™ìŠµ
        svr_pred=svr_dict[label].predict(X_test)

        # ì•™ìƒë¸”ì„ ìœ„í•œ ì˜ˆì¸¡ê°’ ê²°í•©
        y_pred = (xgboost_pred + svr_pred) / 2

        # í‰ê°€: MSE ê³„ì‚°
        mse = mean_squared_error(y_test, y_pred)
        mse_list.append(mse)

    # í‰ê·  MSE ê³„ì‚°
    avg_mse = sum(mse_list) / len(mse_list)
    mse_2_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## SVR+RF"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
import xgboost as xgb

# rf_dict = {}
# svr_dict = {}
mse_3_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold ê°ì²´ ìƒì„±
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # ì¢…ì† ë³€ìˆ˜ ë° ë…ë¦½ ë³€ìˆ˜ ë¶„ë¦¬
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # ê° foldì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # RandomForest ëª¨ë¸ í•™ìŠµ
        rf_pred=rf_dict[label].predict(X_test)

        # SVR ëª¨ë¸ í•™ìŠµ
        svr_pred=svr_dict[label].predict(X_test)

        # ì•™ìƒë¸”ì„ ìœ„í•œ ì˜ˆì¸¡ê°’ ê²°í•©
        y_pred = (rf_pred + svr_pred) / 2

        # í‰ê°€: MSE ê³„ì‚°
        mse = mean_squared_error(y_test, y_pred)
        mse_list.append(mse)

    # í‰ê·  MSE ê³„ì‚°
    avg_mse = sum(mse_list) / len(mse_list)
    mse_3_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## XGB+SVR+RF"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
import xgboost as xgb

# xgboost_dict = {}
# rf_dict = {}
# svr_dict = {}
mse_4_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold ê°ì²´ ìƒì„±
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # ì¢…ì† ë³€ìˆ˜ ë° ë…ë¦½ ë³€ìˆ˜ ë¶„ë¦¬
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # ê° foldì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # # XGBoost ëª¨ë¸ í•™ìŠµ
        # xgboost = xgb.XGBRegressor()
        # xgboost.fit(X_train, y_train)
        # xgboost_pred = xgboost.predict(X_test)
        xgboost_pred=xgboost_dict[label].predict(X_test)

        # # RandomForest ëª¨ë¸ í•™ìŠµ
        # rf = RandomForestRegressor(n_estimators=100, random_state=42)
        # rf.fit(X_train, y_train)
        # rf_pred = rf.predict(X_test)
        rf_pred=rf_dict[label].predict(X_test)

        # # SVR ëª¨ë¸ í•™ìŠµ
        # svr_rbf = SVR(kernel='rbf', C=100, gamma=0.5, epsilon=.1)
        # svr_rbf.fit(X_train, y_train)
        # svr_pred = svr_rbf.predict(X_test)
        svr_pred=svr_dict[label].predict(X_test)

        # ì•™ìƒë¸”ì„ ìœ„í•œ ì˜ˆì¸¡ê°’ ê²°í•©
        y_pred = (xgboost_pred+rf_pred + svr_pred) / 3

        # í‰ê°€: MSE ê³„ì‚°
        mse = mean_squared_error(y_test, y_pred)
        mse_list.append(mse)

    # í‰ê·  MSE ê³„ì‚°
    avg_mse = sum(mse_list) / len(mse_list)
    mse_4_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""# ì‹œê³„ì—´ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ ì˜ˆì¸¡"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
data = pd.read_csv("/content/drive/MyDrive/BME/modified_data2.csv")

from sklearn.model_selection import train_test_split

# ê° í™˜ìì˜ ê³ ìœ í•œ patient_id í™•ì¸
unique_patients = data['patient_id'].unique()

# ê° í™˜ìë¥¼ train ë° test ì„¸íŠ¸ë¡œ ë¶„í• 
train_patients, test_patients = train_test_split(unique_patients, test_size=0.2, random_state=42)

# train ë° test ì„¸íŠ¸ì— ì†í•˜ëŠ” ì¸ë±ìŠ¤ ì¶”ì¶œ
train_idx = data['patient_id'].isin(train_patients)
test_idx = data['patient_id'].isin(test_patients)

# train ë° test ë°ì´í„° ë¶„í• 
train_data = data[train_idx]
test_data = data[test_idx]

#print(train_data,test_data)

train_data.isnull().any()

"""## UPDRS1"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd

# ê° í™˜ìì˜ ë°ì´í„°ë¥¼ ë”°ë¡œ ì²˜ë¦¬í•˜ì—¬ ì‹œê³„ì—´ í˜•íƒœë¡œ ë§Œë“¤ê¸°
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_1'ì„ í¬í•¨í•œ ë°ì´í„°ì…‹ì—ì„œ í•„ìš”í•œ í”¼ì²˜ë§Œ ì„ íƒ
features = ['FIYGGC(UniMod_4)GGNR', 'P36980', 'NVVYTC(UniMod_4)NEGYSLIGNPVAR', 'AGLAASLAGPHSIVGR', 'KYLYEIAR',
            'FNKPFVFLM(UniMod_35)IEQNTK', 'Q9NYU2', 'LLELTGPK', 'C(UniMod_4)FSGQC(UniMod_4)ISK', 'GLGEISAASEFK',
            'VVEESELAR', 'P00748', 'GKRPYQEGTPC(UniMod_4)SQC(UniMod_4)PSGYHC(UniMod_4)K', 'LVYPSC(UniMod_4)EEK',
            'LEEQAQQIR', 'DALSSVQESQVAQQAR', 'LEPGQQEEYYR', 'TATSEYQTFFNPR', 'HYTNPSQDVTVPC(UniMod_4)PVPPPPPC(UniMod_4)C(UniMod_4)HPR',
            'VGGVQSLGGTGALR', 'LGPLVEQGR', 'C(UniMod_4)MC(UniMod_4)PAENPGC(UniMod_4)R', 'SGIEC(UniMod_4)QLWR',
            'GC(UniMod_4)PTEEGC(UniMod_4)GER', 'M(UniMod_35)ADEAGSEADHEGTHSTKR', 'SIVVSPILIPENQR', 'MKYWGVASFLQK',
            'FFLC(UniMod_4)QVAGDAK', 'P06310', 'GNSYFMVEVK', 'GATLALTQVTPQDER', 'EILSVDC(UniMod_4)STNNPSQAK',
            'P17174', 'NILDRQDPPSVVVTSHQAPGEK', 'GLSAEPGWQAK', 'LLPAQLPAEKEVGPPLPQEAVPLQK', 'KTLLSNLEEAKK',
            'ADSGEGDFLAEGGGVR', 'FTILDSQGK', 'C(UniMod_4)LVEKGDVAFVKHQTVPQNTGGK', 'SC(UniMod_4)VGETTESTQC(UniMod_4)EDEELEHLR',
            'LETPDFQLFK', 'ILGPLSYSK', 'P20774', 'KVESELIKPINPR', 'GAAPPKQEFLDIEDP', 'P16070', 'SVPMVPPGIK',
            'DKETC(UniMod_4)FAEEGKK', 'P27169', 'updrs_1']


train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)


# ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, len(features) - 1), return_sequences=True))  # ê° ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ Noneìœ¼ë¡œ ì„¤ì •
model.add(Dropout(0.2))  # ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³ , í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤.
adam = Adam(learning_rate=0.001)
model.compile(optimizer=adam, loss='mean_squared_error')

# ëª¨ë¸ í›ˆë ¨
for sequence in train_patient_sequences:
    X_train = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_train = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_train = np.expand_dims(y_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    model.fit(X_train, y_train, epochs=50, batch_size=16)



# ëª¨ë¸ ì˜ˆì¸¡
predictions = []
true_values = []

for sequence in test_patient_sequences:
    X_test = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_test = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # í˜„ì¬ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œê°’ì´ NaNì´ ì•„ë‹ˆë©´ true_valuesì— ì¶”ê°€í•©ë‹ˆë‹¤.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# MSE ê³„ì‚°
predicted_values = np.concatenate(predictions)
mse = np.mean(np.square(np.array(true_values) - predicted_values))
print("Mean Squared Error:", mse)

# R^2 ìŠ¤ì½”ì–´ ê³„ì‚°
ss_res = np.sum(np.square(true_values - predicted_values))
ss_tot = np.sum(np.square(true_values - np.mean(true_values)))
r2_score = 1 - (ss_res / ss_tot)
print("R^2 Score:", r2_score)

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer

# ê° í™˜ìì˜ ë°ì´í„°ë¥¼ ë”°ë¡œ ì²˜ë¦¬í•˜ì—¬ ì‹œê³„ì—´ í˜•íƒœë¡œ ë§Œë“¤ê¸°
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_1'ì„ í¬í•¨í•œ ë°ì´í„°ì…‹ì—ì„œ í•„ìš”í•œ í”¼ì²˜ë§Œ ì„ íƒ
features = ['FIYGGC(UniMod_4)GGNR', 'P36980', 'NVVYTC(UniMod_4)NEGYSLIGNPVAR', 'AGLAASLAGPHSIVGR', 'KYLYEIAR',
            'FNKPFVFLM(UniMod_35)IEQNTK', 'Q9NYU2', 'LLELTGPK', 'C(UniMod_4)FSGQC(UniMod_4)ISK', 'GLGEISAASEFK',
            'VVEESELAR', 'P00748', 'GKRPYQEGTPC(UniMod_4)SQC(UniMod_4)PSGYHC(UniMod_4)K', 'LVYPSC(UniMod_4)EEK',
            'LEEQAQQIR', 'DALSSVQESQVAQQAR', 'LEPGQQEEYYR', 'TATSEYQTFFNPR', 'HYTNPSQDVTVPC(UniMod_4)PVPPPPPC(UniMod_4)C(UniMod_4)HPR',
            'VGGVQSLGGTGALR', 'LGPLVEQGR', 'C(UniMod_4)MC(UniMod_4)PAENPGC(UniMod_4)R', 'SGIEC(UniMod_4)QLWR',
            'GC(UniMod_4)PTEEGC(UniMod_4)GER', 'M(UniMod_35)ADEAGSEADHEGTHSTKR', 'SIVVSPILIPENQR', 'MKYWGVASFLQK',
            'FFLC(UniMod_4)QVAGDAK', 'P06310', 'GNSYFMVEVK', 'GATLALTQVTPQDER', 'EILSVDC(UniMod_4)STNNPSQAK',
            'P17174', 'NILDRQDPPSVVVTSHQAPGEK', 'GLSAEPGWQAK', 'LLPAQLPAEKEVGPPLPQEAVPLQK', 'KTLLSNLEEAKK',
            'ADSGEGDFLAEGGGVR', 'FTILDSQGK', 'C(UniMod_4)LVEKGDVAFVKHQTVPQNTGGK', 'SC(UniMod_4)VGETTESTQC(UniMod_4)EDEELEHLR',
            'LETPDFQLFK', 'ILGPLSYSK', 'P20774', 'KVESELIKPINPR', 'GAAPPKQEFLDIEDP', 'P16070', 'SVPMVPPGIK',
            'DKETC(UniMod_4)FAEEGKK', 'P27169', 'updrs_1']

train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ë° NaN ê°’ ëŒ€ì²´
scaler = StandardScaler()
imputer = SimpleImputer(strategy='mean')
scaled_train_sequences = [scaler.fit_transform(imputer.fit_transform(seq)) for seq in train_patient_sequences]
scaled_test_sequences = [scaler.transform(imputer.transform(seq)) for seq in test_patient_sequences]

# PCA ì ìš© (n_components ì„¤ì •)
min_samples = min([seq.shape[0] for seq in scaled_train_sequences])  # ê° ì‹œí€€ìŠ¤ì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
n_components = min(10, min_samples - 1)  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ì— ë§ì¶”ì–´ n_componentsë¥¼ ì„¤ì •

pca = PCA(n_components=n_components)
pca_train_sequences = [pca.fit_transform(seq) for seq in scaled_train_sequences]
pca_test_sequences = [pca.transform(seq) for seq in scaled_test_sequences]

# LSTM ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, pca.n_components_), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³ , í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤.
adam = Adam(learning_rate=0.001)
model.compile(optimizer=adam, loss='mean_squared_error')

# ëª¨ë¸ í›ˆë ¨
for sequence in pca_train_sequences:
    X_train = sequence[:-1, :]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_train = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_train = np.expand_dims(y_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# ëª¨ë¸ ì˜ˆì¸¡
predictions = []
true_values = []

for sequence in pca_test_sequences:
    X_test = sequence[:-1, :]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_test = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # í˜„ì¬ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œê°’ì´ NaNì´ ì•„ë‹ˆë©´ true_valuesì— ì¶”ê°€í•©ë‹ˆë‹¤.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error, explained_variance_score, mean_squared_log_error


# MSE (Mean Squared Error)
mse = mean_squared_error(true_values, predicted_values)
print("Mean Squared Error (MSE):", mse)

# MAE (Mean Absolute Error)
mae = mean_absolute_error(true_values, predicted_values)
print("Mean Absolute Error (MAE):", mae)

# MAPE (Mean Absolute Percentage Error)
mape = mean_absolute_percentage_error(true_values, predicted_values)
print("Mean Absolute Percentage Error (MAPE):", mape)

# RÂ² ìŠ¤ì½”ì–´ (ê²°ì • ê³„ìˆ˜, Coefficient of Determination)
r2 = r2_score(true_values, predicted_values)
print("R^2 Score:", r2)

# RMSE (Root Mean Squared Error)
rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)

# Adjusted RÂ² (ì¡°ì •ëœ ê²°ì • ê³„ìˆ˜)
n = len(true_values)
p = 5  # ì˜ˆì œì—ì„œëŠ” ì„¤ëª… ë³€ìˆ˜ì˜ ìˆ˜ë¥¼ 1ë¡œ ê°€ì •
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
print("Adjusted R^2:", adjusted_r2)

# MSLE (Mean Squared Logarithmic Error)
msle = mean_squared_log_error(true_values, predicted_values)
print("Mean Squared Logarithmic Error (MSLE):", msle)

# Explained Variance Score (ì„¤ëª…ëœ ë¶„ì‚° ì ìˆ˜)
explained_variance = explained_variance_score(true_values, predicted_values)
print("Explained Variance Score:", explained_variance)

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error, explained_variance_score

# MSE ê³„ì‚°
predicted_values = np.concatenate(predictions)
mse = np.mean(np.square(np.array(true_values) - predicted_values))
print("Mean Squared Error:", mse)

# í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (Mean Absolute Error, MAE)
mae = mean_absolute_error(true_values, predicted_values)
print("Mean Absolute Error (MAE):", mae)

# í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨ (Mean Absolute Percentage Error, MAPE)
mape = mean_absolute_percentage_error(true_values, predicted_values)
print("Mean Absolute Percentage Error (MAPE):", mape)

# RÂ² ìŠ¤ì½”ì–´ (ê²°ì • ê³„ìˆ˜, Coefficient of Determination)
r2 = r2_score(true_values, predicted_values)
print("R^2 Score:", r2)

# Root Mean Squared Error (RMSE)
rmse = np.sqrt(mean_squared_error(true_values, predicted_values))
print("Root Mean Squared Error (RMSE):", rmse)

# Explained Variance Score
explained_variance = explained_variance_score(true_values, predicted_values)
print("Explained Variance Score:", explained_variance)

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd

# ê° í™˜ìì˜ ë°ì´í„°ë¥¼ ë”°ë¡œ ì²˜ë¦¬í•˜ì—¬ ì‹œê³„ì—´ í˜•íƒœë¡œ ë§Œë“¤ê¸°
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_1'ì„ í¬í•¨í•œ ë°ì´í„°ì…‹ì—ì„œ í•„ìš”í•œ í”¼ì²˜ë§Œ ì„ íƒ
features = ['FIYGGC(UniMod_4)GGNR', 'P36980', 'NVVYTC(UniMod_4)NEGYSLIGNPVAR', 'AGLAASLAGPHSIVGR', 'KYLYEIAR',
            'FNKPFVFLM(UniMod_35)IEQNTK', 'Q9NYU2', 'LLELTGPK', 'C(UniMod_4)FSGQC(UniMod_4)ISK', 'GLGEISAASEFK',
            'VVEESELAR', 'P00748', 'GKRPYQEGTPC(UniMod_4)SQC(UniMod_4)PSGYHC(UniMod_4)K', 'LVYPSC(UniMod_4)EEK',
            'LEEQAQQIR', 'DALSSVQESQVAQQAR', 'LEPGQQEEYYR', 'TATSEYQTFFNPR', 'HYTNPSQDVTVPC(UniMod_4)PVPPPPPC(UniMod_4)C(UniMod_4)HPR',
            'VGGVQSLGGTGALR', 'LGPLVEQGR', 'C(UniMod_4)MC(UniMod_4)PAENPGC(UniMod_4)R', 'SGIEC(UniMod_4)QLWR',
            'GC(UniMod_4)PTEEGC(UniMod_4)GER', 'M(UniMod_35)ADEAGSEADHEGTHSTKR', 'SIVVSPILIPENQR', 'MKYWGVASFLQK',
            'FFLC(UniMod_4)QVAGDAK', 'P06310', 'GNSYFMVEVK', 'GATLALTQVTPQDER', 'EILSVDC(UniMod_4)STNNPSQAK',
            'P17174', 'NILDRQDPPSVVVTSHQAPGEK', 'GLSAEPGWQAK', 'LLPAQLPAEKEVGPPLPQEAVPLQK', 'KTLLSNLEEAKK',
            'ADSGEGDFLAEGGGVR', 'FTILDSQGK', 'C(UniMod_4)LVEKGDVAFVKHQTVPQNTGGK', 'SC(UniMod_4)VGETTESTQC(UniMod_4)EDEELEHLR',
            'LETPDFQLFK', 'ILGPLSYSK', 'P20774', 'KVESELIKPINPR', 'GAAPPKQEFLDIEDP', 'P16070', 'SVPMVPPGIK',
            'DKETC(UniMod_4)FAEEGKK', 'P27169', 'updrs_1']


train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)

from keras.optimizers import RMSprop

# ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(LSTM(units=64, input_shape=(None, len(features) - 1), return_sequences=True))  # ê° ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ Noneìœ¼ë¡œ ì„¤ì •
model.add(Dropout(0.3))  # ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
model.add(LSTM(units=64, return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(units=64))
model.add(Dropout(0.3))
model.add(Dense(units=1))

# RMSprop optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³ , í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤.
rmsprop = RMSprop(learning_rate=0.0005)
model.compile(optimizer=rmsprop, loss='mean_squared_error')

# ëª¨ë¸ í›ˆë ¨
for sequence in train_patient_sequences:
    X_train = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_train = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_train = np.expand_dims(y_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    model.fit(X_train, y_train, epochs=50, batch_size=32)

# ëª¨ë¸ ì˜ˆì¸¡ ë° í‰ê°€ëŠ” ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ì§„í–‰í•©ë‹ˆë‹¤.


# ëª¨ë¸ ì˜ˆì¸¡
predictions = []
true_values = []

for sequence in test_patient_sequences:
    X_test = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_test = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # í˜„ì¬ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œê°’ì´ NaNì´ ì•„ë‹ˆë©´ true_valuesì— ì¶”ê°€í•©ë‹ˆë‹¤.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error, explained_variance_score, mean_squared_log_error


# MSE (Mean Squared Error)
mse = mean_squared_error(true_values, predicted_values)
print("Mean Squared Error (MSE):", mse)

# MAE (Mean Absolute Error)
mae = mean_absolute_error(true_values, predicted_values)
print("Mean Absolute Error (MAE):", mae)

# MAPE (Mean Absolute Percentage Error)
mape = mean_absolute_percentage_error(true_values, predicted_values)
print("Mean Absolute Percentage Error (MAPE):", mape)

# RÂ² ìŠ¤ì½”ì–´ (ê²°ì • ê³„ìˆ˜, Coefficient of Determination)
r2 = r2_score(true_values, predicted_values)
print("R^2 Score:", r2)

# RMSE (Root Mean Squared Error)
rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)

# Adjusted RÂ² (ì¡°ì •ëœ ê²°ì • ê³„ìˆ˜)
n = len(true_values)
p = 1  # ì˜ˆì œì—ì„œëŠ” ì„¤ëª… ë³€ìˆ˜ì˜ ìˆ˜ë¥¼ 1ë¡œ ê°€ì •
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
print("Adjusted R^2:", adjusted_r2)

# MSLE (Mean Squared Logarithmic Error)
msle = mean_squared_log_error(true_values, predicted_values)
print("Mean Squared Logarithmic Error (MSLE):", msle)

# Explained Variance Score (ì„¤ëª…ëœ ë¶„ì‚° ì ìˆ˜)
explained_variance = explained_variance_score(true_values, predicted_values)
print("Explained Variance Score:", explained_variance)

"""## UPDRS2"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd

# ê° í™˜ìì˜ ë°ì´í„°ë¥¼ ë”°ë¡œ ì²˜ë¦¬í•˜ì—¬ ì‹œê³„ì—´ í˜•íƒœë¡œ ë§Œë“¤ê¸°
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_2'ì„ í¬í•¨í•œ ë°ì´í„°ì…‹ì—ì„œ í•„ìš”í•œ í”¼ì²˜ë§Œ ì„ íƒ
features =['FIYGGC(UniMod_4)GGNR', 'LQDLYSIVR', 'MKYWGVASFLQK', 'P30086', 'C(UniMod_4)LPVTAPENGK', 'ETYGEMADC(UniMod_4)C(UniMod_4)AK', 'LEPGQQEEYYR', 'GLSAEPGWQAK', 'KYFIDFVAR',
           'M(UniMod_35)YLGYEYVTAIR', 'EQLSLLDRFTEDAKR', 'GAQTQTEEEMTR', 'P07998', 'NPDSSTTGPWC(UniMod_4)YTTDPTVR', 'SC(UniMod_4)VGETTESTQC(UniMod_4)EDEELEHLR',
           'SC(UniMod_4)DKTHTC(UniMod_4)PPC(UniMod_4)PAPELLGGPSVFLFPPKPK', 'LVNEVTEFAK', 'LSKELQAAQAR', 'P19827', 'ALANSLAC(UniMod_4)QGK', 'TSAHGNVAEGETKPDPDVTER',
           'HYEGSTVPEK', 'VKDISEVVTPR', 'Q9NYU2', 'THLGEALAPLSK', 'P01591', 'EDC(UniMod_4)NELPPRR', 'AGC(UniMod_4)VAESTAVC(UniMod_4)R', 'AGLAASLAGPHSIVGR', 'P07333',
           'EHVAHLLFLR', 'P01034', 'QVVAGLNFR', 'KVESELIKPINPR', 'KTLLSNLEEAK', 'P36980', 'HYTNPSQDVTVPC(UniMod_4)PVPPPPPC(UniMod_4)C(UniMod_4)HPR', 'GWVTDGFSSLK',
           'VLLDGVQNPR', 'HLDSVLQQLQTEVYR', 'QRQEELC(UniMod_4)LAR', 'IGDQWDKQHDMGHMMR', 'YGLVTYATYPK', 'VDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSK', 'NFPPSQDASGDLYTTSSQLTLPATQC(UniMod_4)LAGK',
           'SSGLVSNAPGVQIR', 'LTASAPGYLAITK', 'KQINDYVEKGTQGK', 'ADSGEGDFLAEGGGVR', 'TLKIENVSYQDKGNYR','updrs_2']

train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)

# ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, len(features) - 1), return_sequences=True))  # ê° ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ Noneìœ¼ë¡œ ì„¤ì •
model.add(Dropout(0.2))  # ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³ , í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤.
adam = Adam(learning_rate=0.001)
model.compile(optimizer=adam, loss='mean_squared_error')

# ëª¨ë¸ í›ˆë ¨
for sequence in train_patient_sequences:
    X_train = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_train = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_train = np.expand_dims(y_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    model.fit(X_train, y_train, epochs=50, batch_size=16)


predictions = []
true_values = []

for sequence in test_patient_sequences:
    X_test = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_test = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # í˜„ì¬ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œê°’ì´ NaNì´ ì•„ë‹ˆë©´ true_valuesì— ì¶”ê°€í•©ë‹ˆë‹¤.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# MSE ê³„ì‚°
predicted_values = np.concatenate(predictions)
mse = np.mean(np.square(np.array(true_values) - predicted_values))
print("Mean Squared Error:", mse)

"""## UPDRS3"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd

# ê° í™˜ìì˜ ë°ì´í„°ë¥¼ ë”°ë¡œ ì²˜ë¦¬í•˜ì—¬ ì‹œê³„ì—´ í˜•íƒœë¡œ ë§Œë“¤ê¸°
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_3'ì„ í¬í•¨í•œ ë°ì´í„°ì…‹ì—ì„œ í•„ìš”í•œ í”¼ì²˜ë§Œ ì„ íƒ
features = ['TPC(UniMod_4)TVSC(UniMod_4)NIPVVSGKEC(UniMod_4)EEIIR', 'QHVVYGPWNLPQSSYSHLTR', 'FIYGGC(UniMod_4)GGNR', 'LLEVPEGR', 'P13521', 'GLPAPIEK',
            'ETYGEMADC(UniMod_4)C(UniMod_4)AK', 'Q6UXD5', 'KLGQSLDC(UniMod_4)NAEVYVVPWEK', 'SDVMYTDWKK', 'LADGGATNQGRVEIFYR', 'MYLGYEYVTAIR',
            'LSYTC(UniMod_4)EGGFR', 'KLVGYLDR', 'VTSIQDWVQK', 'QTHQPPAPNSLIR', 'FM(UniMod_35)ETVAEK', 'KDSGFQM(UniMod_35)NQLR', 'P10643', 'YANC(UniMod_4)HLAR',
            'M(UniMod_35)YLGYEYVTAIR', 'HYEGSTVPEK', 'P25311', 'P08123', 'KMTVTDQVNC(UniMod_4)PK', 'NPDSSTTGPWC(UniMod_4)YTTDPTVR', 'GYPGVQAPEDLEWER',
            'NFPPSQDASGDLYTTSSQLTLPATQC(UniMod_4)LAGK', 'GSPAINVAVHVFR', 'C(UniMod_4)PNPPVQENFDVNKYLGR', 'P01877', 'P04406', 'EGDMLTLFDGDGPSAR', 'VFQEPLFYEAPR',
            'RLGMFNIQHC(UniMod_4)K', 'QFTSSTSYNR', 'EKLQDEDLGFL', 'THLGEALAPLSK', 'YQC(UniMod_4)YC(UniMod_4)YGR', 'DPTFIPAPIQAK', 'VTEIWQEVMQR', 'SDVMYTDWK',
            'IEIPSSVQQVPTIIK', 'YPSLSIHGIEGAFDEPGTK', 'THPHFVIPYR', 'IASFSQNC(UniMod_4)DIYPGKDFVQPPTK', 'RLEGQEEEEDNRDSSMK', 'Q13332', 'TLKIENVSYQDKGNYR', 'C(UniMod_4)FSGQC(UniMod_4)ISK','updrs_3']


train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)

# ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, len(features) - 1), return_sequences=True))  # ê° ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ Noneìœ¼ë¡œ ì„¤ì •
model.add(Dropout(0.2))  # ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³ , í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤.
adam = Adam(learning_rate=0.001)
model.compile(optimizer=adam, loss='mean_squared_error')

# ëª¨ë¸ í›ˆë ¨
for sequence in train_patient_sequences:
    X_train = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_train = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_train = np.expand_dims(y_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# ëª¨ë¸ ì˜ˆì¸¡
predictions = []
true_values = []

for sequence in test_patient_sequences:
    X_test = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_test = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # í˜„ì¬ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œê°’ì´ NaNì´ ì•„ë‹ˆë©´ true_valuesì— ì¶”ê°€í•©ë‹ˆë‹¤.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# MSE ê³„ì‚°
predicted_values = np.concatenate(predictions)
mse = np.mean(np.square(np.array(true_values) - predicted_values))
print("Mean Squared Error:", mse)

"""## UPDRS4"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd

# ê° í™˜ìì˜ ë°ì´í„°ë¥¼ ë”°ë¡œ ì²˜ë¦¬í•˜ì—¬ ì‹œê³„ì—´ í˜•íƒœë¡œ ë§Œë“¤ê¸°
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_4'ì„ í¬í•¨í•œ ë°ì´í„°ì…‹ì—ì„œ í•„ìš”í•œ í”¼ì²˜ë§Œ ì„ íƒ
features = ['VNHVTLSQPK', 'SILENLR', 'STGGISVPGPMGPSGPR', 'M(UniMod_35)LTPEHVFIHPGWK', 'DVLLEAC(UniMod_4)C(UniMod_4)ADGHR', 'AQC(UniMod_4)GGGLLGVR',
            'SVPPSASHVAPTETFTYEWTVPK', 'VC(UniMod_4)PFAGILENGAVR', 'HGNVAEGETKPDPDVTER', 'EHVAHLLFLR', 'KLSSWVLLMK', 'LEPGQQEEYYR', 'EKLQDEDLGFL',
            'C(UniMod_4)PFPSRPDNGFVNYPAKPTLYYK', 'LLDNWDSVTSTFSK', 'Q92520', 'TPSAAYLWVGTGASEAEK', 'PPTSAHGNVAEGETKPDPDVTER', 'WSRPQAPITGYR', 'Q99435',
            'SFQTGLFTAAR', 'MMAVAADTLQR', 'SVPMVPPGIK', 'AADDTWEPFASGK', 'P23083', 'Q99683', 'FIYGGC(UniMod_4)GGNR', 'QFTSSTSYNR', 'AYLEEEC(UniMod_4)PATLRK',
            'FVVTDGGITR', 'VPFDAATLHTSTAMAAQHGMDDDGTGQK', 'GC(UniMod_4)PTEEGC(UniMod_4)GER', 'QKVEPLRAELQEGAR', 'RTHLPEVFLSK', 'Q92876', 'AIGAVPLIQGEYMIPC(UniMod_4)EK',
            'VYC(UniMod_4)DMNTENGGWTVIQNR', 'TSAHGNVAEGETKPDPDVTER', 'AGLAASLAGPHSIVGR', 'AGAAAGGPGVSGVC(UniMod_4)VC(UniMod_4)K', 'RLEAGDHPVELLAR', 'EDC(UniMod_4)NELPPRR',
            'GSPSGEVSHPR', 'C(UniMod_4)LAPLEGAR', 'VTGVVLFR', 'GNSYFMVEVK', 'LDEVKEQVAEVR', 'ATEDEGSEQKIPEATNR', 'SVIPSDGPSVAC(UniMod_4)VK', 'C(UniMod_4)VC(UniMod_4)PVSNAMC(UniMod_4)R','updrs_4']


train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)

# ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, len(features) - 1), return_sequences=True))
model.add(Dropout(0.2))  # ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

adam = Adam(learning_rate=0.001)
model.compile(optimizer=adam, loss='mean_squared_error')

# ëª¨ë¸ í›ˆë ¨
for sequence in train_patient_sequences:
    X_train = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_train = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_train = np.expand_dims(y_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# ëª¨ë¸ ì˜ˆì¸¡
predictions = []
true_values = []

for sequence in test_patient_sequences:
    X_test = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_test = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # í˜„ì¬ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œê°’ì´ NaNì´ ì•„ë‹ˆë©´ true_valuesì— ì¶”ê°€í•©ë‹ˆë‹¤.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# MSE ê³„ì‚°
predicted_values = np.concatenate(predictions)
mse = np.mean(np.square(np.array(true_values) - predicted_values))
print("Mean Squared Error:", mse)

"""# **ì° PCA**

**UPDRS1: MSE=14**
"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
updrs1 = pd.read_csv("/content/drive/MyDrive/BME/updrs1.csv")

# ê° í™˜ìì˜ ê³ ìœ í•œ patient_id í™•ì¸
unique_patients = updrs1['patient_id'].unique()

# ê° í™˜ìë¥¼ train ë° test ì„¸íŠ¸ë¡œ ë¶„í• 
train_patients, test_patients = train_test_split(unique_patients, test_size=0.2, random_state=42)

# train ë° test ì„¸íŠ¸ì— ì†í•˜ëŠ” ì¸ë±ìŠ¤ ì¶”ì¶œ
train_idx = updrs1['patient_id'].isin(train_patients)
test_idx = updrs1['patient_id'].isin(test_patients)

# train ë° test ë°ì´í„° ë¶„í• 
train_data = updrs1[train_idx]
test_data = updrs1[test_idx]

# Extract patient IDs
train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

# Drop non-feature columns and the target column
train_features = train_data.drop(columns=['patient_id','visit_id','visit_month','updrs_1']).columns
test_features = test_data.drop(columns=['patient_id','visit_id','visit_month','updrs_1']).columns

# Extract target columns
train_targets = train_data['updrs_1']
test_targets = test_data['updrs_1']

train_patient_sequences = []
test_patient_sequences = []
train_target_sequences = []
test_target_sequences = []

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][train_features].values
    patient_target = train_data[train_data['patient_id'] == patient_id]['updrs_1'].values
    train_patient_sequences.append(patient_data)
    train_target_sequences.append(patient_target)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][test_features].values
    patient_target = test_data[test_data['patient_id'] == patient_id]['updrs_1'].values
    test_patient_sequences.append(patient_data)
    test_target_sequences.append(patient_target)

# ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚° (ì¤‘ê°„ ì°¨ì›ì˜ ìµœëŒ€ ê¸¸ì´)
max_seq_len = max(len(patient) for patient in train_patient_sequences)
max_seq_len_t = max(len(patient) for patient in test_patient_sequences)

print(max_seq_len)
print(max_seq_len_t)

# ê° ì‹œí€€ìŠ¤ë¥¼ ë™ì¼í•œ ê¸¸ì´ë¡œ íŒ¨ë”©
data_3d_padded = pad_sequences(train_patient_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testdata_3d_padded = pad_sequences(test_patient_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# íƒ€ê²Ÿ ì‹œí€€ìŠ¤ë¥¼ ë™ì¼í•œ ê¸¸ì´ë¡œ íŒ¨ë”©
target_3d_padded = pad_sequences(train_target_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testtarget_3d_padded = pad_sequences(test_target_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# 3ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
data_3d_padded = np.array(data_3d_padded)
testdata_3d_padded = np.array(testdata_3d_padded)
target_3d_padded = np.array(target_3d_padded)
testtarget_3d_padded = np.array(testtarget_3d_padded)

print("3D array shape:", data_3d_padded.shape)
print("3D array shape:", testdata_3d_padded.shape)
print("Target 3D array shape:", target_3d_padded.shape)
print("Test Target 3D array shape:", testtarget_3d_padded.shape)

# 3ì°¨ì› ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (198*max_seq_len, 1196)
num_samples, num_timesteps, num_features = data_3d_padded.shape
num_samplest, num_timestepst, num_featurest = testdata_3d_padded.shape

data_2d = data_3d_padded.reshape(num_samples * num_timesteps, num_features)
testdata_2d = testdata_3d_padded.reshape(num_samplest * num_timestepst, num_featurest)

# ë°ì´í„° í‘œì¤€í™”
scaler = StandardScaler()
data_2d_scaled = scaler.fit_transform(data_2d)
testdata_2d_scaled = scaler.fit_transform(testdata_2d)

# PCA ëª¨ë¸ ìƒì„±
pca = PCA(n_components=100)

# PCA ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì••ì¶•
data_2d_compressed = pca.fit_transform(data_2d_scaled)
testdata_2d_compressed = pca.fit_transform(testdata_2d_scaled)

# ì••ì¶•ëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œ 3ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (198, max_seq_len, 100)
pca_3d_sequences = data_2d_compressed.reshape(num_samples, num_timesteps, 100)
tpca_3d_sequences = testdata_2d_compressed.reshape(num_samplest, num_timestepst, 100)

# íƒ€ê²Ÿ ì»¬ëŸ¼ ì¶”ê°€ (198, max_seq_len, 101)
pca_3d_sequences_with_target = np.concatenate([pca_3d_sequences, target_3d_padded[..., np.newaxis]], axis=-1)
tpca_3d_sequences_with_target = np.concatenate([tpca_3d_sequences, testtarget_3d_padded[..., np.newaxis]], axis=-1)

print("PCA 3D sequences with target shape:", pca_3d_sequences_with_target.shape)
print("Test PCA 3D sequences with target shape:", tpca_3d_sequences_with_target.shape)

# LSTM ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, 101), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³ , í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤.
adam = Adam(learning_rate=0.01)
model.compile(optimizer=adam, loss='mean_squared_error')

# ëª¨ë¸ í›ˆë ¨
for sequence in pca_3d_sequences_with_target:
    X_train = sequence[:-1, :]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_train = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_train = np.expand_dims(y_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# ëª¨ë¸ ì˜ˆì¸¡
predictions = []
true_values = []

for sequence in tpca_3d_sequences_with_target:
    X_test = sequence[:-1, :]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_test = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # í˜„ì¬ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œê°’ì´ NaNì´ ì•„ë‹ˆë©´ true_valuesì— ì¶”ê°€í•©ë‹ˆë‹¤.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# Flatten the predictions list and true_values list
predicted_values = np.concatenate(predictions)
true_values = np.array(true_values)

# Evaluation metrics
mse = mean_squared_error(true_values, predicted_values)
print("Mean Squared Error (MSE):", mse)

mae = mean_absolute_error(true_values, predicted_values)
print("Mean Absolute Error (MAE):", mae)

mape = mean_absolute_percentage_error(true_values, predicted_values)
print("Mean Absolute Percentage Error (MAPE):", mape)

r2 = r2_score(true_values, predicted_values)
print("R^2 Score:", r2)

rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)

"""**UPDRS2 : MSE=20**"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
updrs2 = pd.read_csv("/content/drive/MyDrive/BME/updrs2.csv")

# ê° í™˜ìì˜ ê³ ìœ í•œ patient_id í™•ì¸
unique_patients = updrs2['patient_id'].unique()

# ê° í™˜ìë¥¼ train ë° test ì„¸íŠ¸ë¡œ ë¶„í• 
train_patients, test_patients = train_test_split(unique_patients, test_size=0.2, random_state=42)

# train ë° test ì„¸íŠ¸ì— ì†í•˜ëŠ” ì¸ë±ìŠ¤ ì¶”ì¶œ
train_idx = updrs2['patient_id'].isin(train_patients)
test_idx = updrs2['patient_id'].isin(test_patients)

# train ë° test ë°ì´í„° ë¶„í• 
train_data = updrs2[train_idx]
test_data = updrs2[test_idx]

# Extract patient IDs
train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

# Drop non-feature columns and the target column
train_features = train_data.drop(columns=['patient_id','visit_id','visit_month','updrs_2']).columns
test_features = test_data.drop(columns=['patient_id','visit_id','visit_month','updrs_2']).columns

# Extract target columns
train_targets = train_data['updrs_2']
test_targets = test_data['updrs_2']

train_patient_sequences = []
test_patient_sequences = []
train_target_sequences = []
test_target_sequences = []

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][train_features].values
    patient_target = train_data[train_data['patient_id'] == patient_id]['updrs_2'].values
    train_patient_sequences.append(patient_data)
    train_target_sequences.append(patient_target)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][test_features].values
    patient_target = test_data[test_data['patient_id'] == patient_id]['updrs_2'].values
    test_patient_sequences.append(patient_data)
    test_target_sequences.append(patient_target)

# ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚° (ì¤‘ê°„ ì°¨ì›ì˜ ìµœëŒ€ ê¸¸ì´)
max_seq_len = max(len(patient) for patient in train_patient_sequences)
max_seq_len_t = max(len(patient) for patient in test_patient_sequences)

print(max_seq_len)
print(max_seq_len_t)

# ê° ì‹œí€€ìŠ¤ë¥¼ ë™ì¼í•œ ê¸¸ì´ë¡œ íŒ¨ë”©
data_3d_padded = pad_sequences(train_patient_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testdata_3d_padded = pad_sequences(test_patient_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# íƒ€ê²Ÿ ì‹œí€€ìŠ¤ë¥¼ ë™ì¼í•œ ê¸¸ì´ë¡œ íŒ¨ë”©
target_3d_padded = pad_sequences(train_target_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testtarget_3d_padded = pad_sequences(test_target_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# 3ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
data_3d_padded = np.array(data_3d_padded)
testdata_3d_padded = np.array(testdata_3d_padded)
target_3d_padded = np.array(target_3d_padded)
testtarget_3d_padded = np.array(testtarget_3d_padded)

print("3D array shape:", data_3d_padded.shape)
print("3D array shape:", testdata_3d_padded.shape)
print("Target 3D array shape:", target_3d_padded.shape)
print("Test Target 3D array shape:", testtarget_3d_padded.shape)

# 3ì°¨ì› ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (198*max_seq_len, 1196)
num_samples, num_timesteps, num_features = data_3d_padded.shape
num_samplest, num_timestepst, num_featurest = testdata_3d_padded.shape

data_2d = data_3d_padded.reshape(num_samples * num_timesteps, num_features)
testdata_2d = testdata_3d_padded.reshape(num_samplest * num_timestepst, num_featurest)

# ë°ì´í„° í‘œì¤€í™”
scaler = StandardScaler()
data_2d_scaled = scaler.fit_transform(data_2d)
testdata_2d_scaled = scaler.fit_transform(testdata_2d)

# PCA ëª¨ë¸ ìƒì„±
pca = PCA(n_components=100)

# PCA ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì••ì¶•
data_2d_compressed = pca.fit_transform(data_2d_scaled)
testdata_2d_compressed = pca.fit_transform(testdata_2d_scaled)

# ì••ì¶•ëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œ 3ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (198, max_seq_len, 100)
pca_3d_sequences = data_2d_compressed.reshape(num_samples, num_timesteps, 100)
tpca_3d_sequences = testdata_2d_compressed.reshape(num_samplest, num_timestepst, 100)

# íƒ€ê²Ÿ ì»¬ëŸ¼ ì¶”ê°€ (198, max_seq_len, 101)
pca_3d_sequences_with_target = np.concatenate([pca_3d_sequences, target_3d_padded[..., np.newaxis]], axis=-1)
tpca_3d_sequences_with_target = np.concatenate([tpca_3d_sequences, testtarget_3d_padded[..., np.newaxis]], axis=-1)

print("PCA 3D sequences with target shape:", pca_3d_sequences_with_target.shape)
print("Test PCA 3D sequences with target shape:", tpca_3d_sequences_with_target.shape)

# LSTM ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, 101), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³ , í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤.
adam = Adam(learning_rate=0.01)
model.compile(optimizer=adam, loss='mean_squared_error')

# ëª¨ë¸ í›ˆë ¨
for sequence in pca_3d_sequences_with_target:
    X_train = sequence[:-1, :]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_train = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_train = np.expand_dims(y_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# ëª¨ë¸ ì˜ˆì¸¡
predictions = []
true_values = []

for sequence in tpca_3d_sequences_with_target:
    X_test = sequence[:-1, :]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_test = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # í˜„ì¬ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œê°’ì´ NaNì´ ì•„ë‹ˆë©´ true_valuesì— ì¶”ê°€í•©ë‹ˆë‹¤.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# Flatten the predictions list and true_values list
predicted_values = np.concatenate(predictions)
true_values = np.array(true_values)

# Evaluation metrics
mse = mean_squared_error(true_values, predicted_values)
print("Mean Squared Error (MSE):", mse)

mae = mean_absolute_error(true_values, predicted_values)
print("Mean Absolute Error (MAE):", mae)

mape = mean_absolute_percentage_error(true_values, predicted_values)
print("Mean Absolute Percentage Error (MAPE):", mape)

r2 = r2_score(true_values, predicted_values)
print("R^2 Score:", r2)

rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)

"""**UPDRS3: MSE=143**"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
updrs3 = pd.read_csv("/content/drive/MyDrive/BME/updrs3.csv")

# ê° í™˜ìì˜ ê³ ìœ í•œ patient_id í™•ì¸
unique_patients = updrs3['patient_id'].unique()

# ê° í™˜ìë¥¼ train ë° test ì„¸íŠ¸ë¡œ ë¶„í• 
train_patients, test_patients = train_test_split(unique_patients, test_size=0.2, random_state=42)

# train ë° test ì„¸íŠ¸ì— ì†í•˜ëŠ” ì¸ë±ìŠ¤ ì¶”ì¶œ
train_idx = updrs3['patient_id'].isin(train_patients)
test_idx = updrs3['patient_id'].isin(test_patients)

# train ë° test ë°ì´í„° ë¶„í• 
train_data = updrs3[train_idx]
test_data = updrs3[test_idx]

# Extract patient IDs
train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

# Drop non-feature columns and the target column
train_features = train_data.drop(columns=['patient_id','visit_id','visit_month','updrs_3']).columns
test_features = test_data.drop(columns=['patient_id','visit_id','visit_month','updrs_3']).columns

# Extract target columns
train_targets = train_data['updrs_3']
test_targets = test_data['updrs_3']

train_patient_sequences = []
test_patient_sequences = []
train_target_sequences = []
test_target_sequences = []

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][train_features].values
    patient_target = train_data[train_data['patient_id'] == patient_id]['updrs_3'].values
    train_patient_sequences.append(patient_data)
    train_target_sequences.append(patient_target)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][test_features].values
    patient_target = test_data[test_data['patient_id'] == patient_id]['updrs_3'].values
    test_patient_sequences.append(patient_data)
    test_target_sequences.append(patient_target)

# ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚° (ì¤‘ê°„ ì°¨ì›ì˜ ìµœëŒ€ ê¸¸ì´)
max_seq_len = max(len(patient) for patient in train_patient_sequences)
max_seq_len_t = max(len(patient) for patient in test_patient_sequences)

print(max_seq_len)
print(max_seq_len_t)

# ê° ì‹œí€€ìŠ¤ë¥¼ ë™ì¼í•œ ê¸¸ì´ë¡œ íŒ¨ë”©
data_3d_padded = pad_sequences(train_patient_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testdata_3d_padded = pad_sequences(test_patient_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# íƒ€ê²Ÿ ì‹œí€€ìŠ¤ë¥¼ ë™ì¼í•œ ê¸¸ì´ë¡œ íŒ¨ë”©
target_3d_padded = pad_sequences(train_target_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testtarget_3d_padded = pad_sequences(test_target_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# 3ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
data_3d_padded = np.array(data_3d_padded)
testdata_3d_padded = np.array(testdata_3d_padded)
target_3d_padded = np.array(target_3d_padded)
testtarget_3d_padded = np.array(testtarget_3d_padded)

print("3D array shape:", data_3d_padded.shape)
print("3D array shape:", testdata_3d_padded.shape)
print("Target 3D array shape:", target_3d_padded.shape)
print("Test Target 3D array shape:", testtarget_3d_padded.shape)

# 3ì°¨ì› ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (198*max_seq_len, 1196)
num_samples, num_timesteps, num_features = data_3d_padded.shape
num_samplest, num_timestepst, num_featurest = testdata_3d_padded.shape

data_2d = data_3d_padded.reshape(num_samples * num_timesteps, num_features)
testdata_2d = testdata_3d_padded.reshape(num_samplest * num_timestepst, num_featurest)

# ë°ì´í„° í‘œì¤€í™”
scaler = StandardScaler()
data_2d_scaled = scaler.fit_transform(data_2d)
testdata_2d_scaled = scaler.fit_transform(testdata_2d)

# PCA ëª¨ë¸ ìƒì„±
pca = PCA(n_components=100)

# PCA ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì••ì¶•
data_2d_compressed = pca.fit_transform(data_2d_scaled)
testdata_2d_compressed = pca.fit_transform(testdata_2d_scaled)

# ì••ì¶•ëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œ 3ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (198, max_seq_len, 100)
pca_3d_sequences = data_2d_compressed.reshape(num_samples, num_timesteps, 100)
tpca_3d_sequences = testdata_2d_compressed.reshape(num_samplest, num_timestepst, 100)

# íƒ€ê²Ÿ ì»¬ëŸ¼ ì¶”ê°€ (198, max_seq_len, 101)
pca_3d_sequences_with_target = np.concatenate([pca_3d_sequences, target_3d_padded[..., np.newaxis]], axis=-1)
tpca_3d_sequences_with_target = np.concatenate([tpca_3d_sequences, testtarget_3d_padded[..., np.newaxis]], axis=-1)

print("PCA 3D sequences with target shape:", pca_3d_sequences_with_target.shape)
print("Test PCA 3D sequences with target shape:", tpca_3d_sequences_with_target.shape)

# LSTM ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, 101), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³ , í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤.
adam = Adam(learning_rate=0.01)
model.compile(optimizer=adam, loss='mean_squared_error')

# ëª¨ë¸ í›ˆë ¨
for sequence in pca_3d_sequences_with_target:
    X_train = sequence[:-1, :]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_train = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_train = np.expand_dims(y_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# ëª¨ë¸ ì˜ˆì¸¡
predictions = []
true_values = []

for sequence in tpca_3d_sequences_with_target:
    X_test = sequence[:-1, :]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_test = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # í˜„ì¬ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œê°’ì´ NaNì´ ì•„ë‹ˆë©´ true_valuesì— ì¶”ê°€í•©ë‹ˆë‹¤.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# Flatten the predictions list and true_values list
predicted_values = np.concatenate(predictions)
true_values = np.array(true_values)

# Evaluation metrics
mse = mean_squared_error(true_values, predicted_values)
print("Mean Squared Error (MSE):", mse)

mae = mean_absolute_error(true_values, predicted_values)
print("Mean Absolute Error (MAE):", mae)

mape = mean_absolute_percentage_error(true_values, predicted_values)
print("Mean Absolute Percentage Error (MAPE):", mape)

r2 = r2_score(true_values, predicted_values)
print("R^2 Score:", r2)

rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)

"""**UPDRS4: MSE=**"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
updrs4 = pd.read_csv("/content/drive/MyDrive/BME/updrs4.csv")

# ê° í™˜ìì˜ ê³ ìœ í•œ patient_id í™•ì¸
unique_patients = updrs4['patient_id'].unique()

# ê° í™˜ìë¥¼ train ë° test ì„¸íŠ¸ë¡œ ë¶„í• 
train_patients, test_patients = train_test_split(unique_patients, test_size=0.2, random_state=42)

# train ë° test ì„¸íŠ¸ì— ì†í•˜ëŠ” ì¸ë±ìŠ¤ ì¶”ì¶œ
train_idx = updrs4['patient_id'].isin(train_patients)
test_idx = updrs4['patient_id'].isin(test_patients)

# train ë° test ë°ì´í„° ë¶„í• 
train_data = updrs4[train_idx]
test_data = updrs4[test_idx]

# Extract patient IDs
train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

# Drop non-feature columns and the target column
train_features = train_data.drop(columns=['patient_id','visit_id','visit_month','updrs_4']).columns
test_features = test_data.drop(columns=['patient_id','visit_id','visit_month','updrs_4']).columns

# Extract target columns
train_targets = train_data['updrs_4']
test_targets = test_data['updrs_4']

train_patient_sequences = []
test_patient_sequences = []
train_target_sequences = []
test_target_sequences = []

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][train_features].values
    patient_target = train_data[train_data['patient_id'] == patient_id]['updrs_4'].values
    train_patient_sequences.append(patient_data)
    train_target_sequences.append(patient_target)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][test_features].values
    patient_target = test_data[test_data['patient_id'] == patient_id]['updrs_4'].values
    test_patient_sequences.append(patient_data)
    test_target_sequences.append(patient_target)

# ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚° (ì¤‘ê°„ ì°¨ì›ì˜ ìµœëŒ€ ê¸¸ì´)
max_seq_len = max(len(patient) for patient in train_patient_sequences)
max_seq_len_t = max(len(patient) for patient in test_patient_sequences)

print(max_seq_len)
print(max_seq_len_t)

# ê° ì‹œí€€ìŠ¤ë¥¼ ë™ì¼í•œ ê¸¸ì´ë¡œ íŒ¨ë”©
data_3d_padded = pad_sequences(train_patient_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testdata_3d_padded = pad_sequences(test_patient_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# íƒ€ê²Ÿ ì‹œí€€ìŠ¤ë¥¼ ë™ì¼í•œ ê¸¸ì´ë¡œ íŒ¨ë”©
target_3d_padded = pad_sequences(train_target_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testtarget_3d_padded = pad_sequences(test_target_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# 3ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
data_3d_padded = np.array(data_3d_padded)
testdata_3d_padded = np.array(testdata_3d_padded)
target_3d_padded = np.array(target_3d_padded)
testtarget_3d_padded = np.array(testtarget_3d_padded)

print("3D array shape:", data_3d_padded.shape)
print("3D array shape:", testdata_3d_padded.shape)
print("Target 3D array shape:", target_3d_padded.shape)
print("Test Target 3D array shape:", testtarget_3d_padded.shape)

# 3ì°¨ì› ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (198*max_seq_len, 1196)
num_samples, num_timesteps, num_features = data_3d_padded.shape
num_samplest, num_timestepst, num_featurest = testdata_3d_padded.shape

data_2d = data_3d_padded.reshape(num_samples * num_timesteps, num_features)
testdata_2d = testdata_3d_padded.reshape(num_samplest * num_timestepst, num_featurest)

# ë°ì´í„° í‘œì¤€í™”
scaler = StandardScaler()
data_2d_scaled = scaler.fit_transform(data_2d)
testdata_2d_scaled = scaler.fit_transform(testdata_2d)

# PCA ëª¨ë¸ ìƒì„±
pca = PCA(n_components=100)

# PCA ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì••ì¶•
data_2d_compressed = pca.fit_transform(data_2d_scaled)
testdata_2d_compressed = pca.fit_transform(testdata_2d_scaled)

# ì••ì¶•ëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œ 3ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (198, max_seq_len, 100)
pca_3d_sequences = data_2d_compressed.reshape(num_samples, num_timesteps, 100)
tpca_3d_sequences = testdata_2d_compressed.reshape(num_samplest, num_timestepst, 100)

# íƒ€ê²Ÿ ì»¬ëŸ¼ ì¶”ê°€ (198, max_seq_len, 101)
pca_3d_sequences_with_target = np.concatenate([pca_3d_sequences, target_3d_padded[..., np.newaxis]], axis=-1)
tpca_3d_sequences_with_target = np.concatenate([tpca_3d_sequences, testtarget_3d_padded[..., np.newaxis]], axis=-1)

print("PCA 3D sequences with target shape:", pca_3d_sequences_with_target.shape)
print("Test PCA 3D sequences with target shape:", tpca_3d_sequences_with_target.shape)

# LSTM ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, 101), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³ , í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤.
adam = Adam(learning_rate=0.01)
model.compile(optimizer=adam, loss='mean_squared_error')

# ëª¨ë¸ í›ˆë ¨
for sequence in pca_3d_sequences_with_target:
    X_train = sequence[:-1, :]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_train = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_train = np.expand_dims(y_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# ëª¨ë¸ ì˜ˆì¸¡
predictions = []
true_values = []

for sequence in tpca_3d_sequences_with_target:
    X_test = sequence[:-1, :]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_test = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # í˜„ì¬ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œê°’ì´ NaNì´ ì•„ë‹ˆë©´ true_valuesì— ì¶”ê°€í•©ë‹ˆë‹¤.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# Flatten the predictions list and true_values list
predicted_values = np.concatenate(predictions)
true_values = np.array(true_values)

# Evaluation metrics
mse = mean_squared_error(true_values, predicted_values)
print("Mean Squared Error (MSE):", mse)

mae = mean_absolute_error(true_values, predicted_values)
print("Mean Absolute Error (MAE):", mae)

mape = mean_absolute_percentage_error(true_values, predicted_values)
print("Mean Absolute Percentage Error (MAPE):", mape)

r2 = r2_score(true_values, predicted_values)
print("R^2 Score:", r2)

rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)