# -*- coding: utf-8 -*-
"""BME Capstone

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c1vqEGlqlw-oNHchHQOYDC3zXh1Z6Gmw

# 🧸 BME Capstone
:predicting MDS-UPDR scores by using protein and peptide levels

👉 202100805 김시은, 202102238 유희진

# 데이터 전처리
"""

import csv
from collections import defaultdict
import pandas as pd

#Load a dataset into a Pandas DataFrame
train_proteins = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_proteins.csv")
train_peptides = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_peptides.csv")
train_clinical = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_clinical_data.csv")

#Uniprot, peptide 종류 수 확인

unique_Uniprot = train_proteins['UniProt'].nunique()
print("Unique values in 'Uniprot' column:", unique_Uniprot)

unique_peptide=train_peptides['Peptide'].nunique()
print("Unique valeus in 'Peptide' column:", unique_peptide)

# Function to prepare dataset with all the steps mentioned above:
def prepare_dataset(train_proteins, train_peptides):
    # Step 1: Grouping
    df_protein_grouped = train_proteins.groupby(['visit_id','UniProt'])['NPX'].mean().reset_index()
    df_peptide_grouped = train_peptides.groupby(['visit_id','Peptide'])['PeptideAbundance'].mean().reset_index()

    # Step 2: Pivoting
    df_protein = df_protein_grouped.pivot(index='visit_id',columns = 'UniProt', values = 'NPX').rename_axis(columns=None).reset_index()
    df_peptide = df_peptide_grouped.pivot(index='visit_id',columns = 'Peptide', values = 'PeptideAbundance').rename_axis(columns=None).reset_index()

    # Step 3: Merging
    pro_pep_df = df_protein.merge(df_peptide, on = ['visit_id'], how = 'left')

    return pro_pep_df

pro_pep_df = prepare_dataset(train_proteins, train_peptides)
# CSV 파일로 저장
pro_pep_df.to_csv('/content/drive/MyDrive/BME/a_data.csv', index=False)
print(pro_pep_df.head())

import pandas as pd
a_data = pd.read_csv("/content/drive/MyDrive/BME/a_data.csv")
train_clinical = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_clinical_data.csv")

# visit_id를 기준으로 두 파일 병합 (외부 조인)
merged_df = pd.merge(a_data, train_clinical, on='visit_id', how='outer')

# 마지막 열을 제거
df = merged_df.drop(merged_df.columns[-1], axis=1)

# 'patient id'와 'visitmonth' 열을 제거합니다
df = df.drop(['patient_id', 'visit_month'], axis=1)

# 'visit_id' 열을 '_'로 분할하여 'patient_id'와 'visit_month'로 나누기
df[['patient_id', 'visit_month']] = df['visit_id'].str.split('_', expand=True)

# 'patient_id'와 'visit_month' 열의 데이터 타입을 적절히 변환 (옵션)
df['patient_id'] = df['patient_id'].astype(int)
df['visit_month'] = df['visit_month'].astype(int)

# 열 순서 조정하여 맨 뒤에 있는 두 개의 열을 맨 앞으로 가져오기
columns = df.columns.tolist()
new_order = columns[-2:] + columns[:-2]

# 새로운 순서로 데이터프레임 생성
df = df[new_order]

# patient_id를 기준으로 오름차순으로 정렬한 후, visit_month를 기준으로 오름차순으로 정렬합니다
df_sorted = df.sort_values(by=['patient_id', 'visit_month'])

df_sorted

# CSV 파일로 저장합니다

from sklearn.impute import SimpleImputer

# 각 환자의 각 열에 대한 존재하는 값들의 평균을 계산합니다
patient_means = df_sorted.groupby('patient_id').mean()

# null 값을 해당 환자의 평균 값으로 대체합니다
imputer = SimpleImputer(strategy='mean')

feature_list = df_sorted.columns[3:-4]

# null 값만 대체합니다
for col in feature_list:
    mask = df_sorted[col].isnull()  # null 값을 확인합니다
    if mask.any():  # null 값을 포함하는 경우에만 처리합니다
        for patient_id, mean_value in patient_means[col].items():
            df_sorted.loc[(df_sorted['patient_id'] == patient_id) & mask, col] = mean_value  # 해당 환자의 null 값만 대체합니다

df_sorted

#null을 평균값으로 대체하고도 남아있는 null값은 0으로 대체
for col in feature_list:# 특정 열에 있는 null 값을 0으로 대체 (예를 들어, 'protein1' 열)
  df_sorted[col].fillna(0, inplace=True)

# 결과 확인
print(df_sorted.isnull().sum())

df_sorted.to_csv('/content/drive/MyDrive/BME/modified_data2.csv', index=False)

"""#  시계열 정보를 반영하지 않고 예측

## XGBoost
"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

xgboost_dict = {}
mse_xg_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold 객체 생성
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # 종속 변수 및 독립 변수 분리
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # 각 fold에 대해 모델 학습 및 평가
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # XGBoost 모델 학습
        xgboost = xgb.XGBRegressor()
        xgboost.fit(X_train, y_train)
        xgboost_pred = xgboost.predict(X_test)

        # 모델 저장
        xgboost_dict[label]=xgboost

        # 평가: MSE 계산
        mse = mean_squared_error(y_test, xgboost_pred)
        mse_list.append(mse)

    # 평균 MSE 계산
    avg_mse = sum(mse_list) / len(mse_list)
    mse_xg_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## RF"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor

rf_dict = {}
mse_rf_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold 객체 생성
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # 종속 변수 및 독립 변수 분리
    X = data.drop(['visit_id','visit_month','patient_id','updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # 각 fold에 대해 모델 학습 및 평가
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # RandomForest 모델 학습
        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        rf.fit(X_train, y_train)
        rf_pred = rf.predict(X_test)

        # 모델 저장
        rf_dict[label]= rf

        # 평가: MSE 계산
        mse = mean_squared_error(y_test, rf_pred)
        mse_list.append(mse)

    # 평균 MSE 계산
    avg_mse = sum(mse_list) / len(mse_list)
    mse_rf_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## SVR"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR

svr_dict={}
mse_svr_dict={}

target = ['updrs_1','updrs_2','updrs_3','updrs_4']

# KFold 객체 생성
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # 종속 변수 및 독립 변수 분리
    X = data.drop(['visit_id','visit_month','patient_id','updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # 각 fold에 대해 모델 학습 및 평가
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # SVR 모델 학습
        svr_rbf = SVR(kernel='rbf', C=100, gamma=0.5, epsilon=.1)
        svr_rbf.fit(X_train, y_train)
        svr_pred=svr_rbf.predict(X_test)

        # 모델 저장
        svr_dict[label]=svr_rbf

        # 평가: MSE 계산
        mse = mean_squared_error(y_test, svr_pred)
        mse_list.append(mse)

    # 평균 MSE 계산
    avg_mse = sum(mse_list) / len(mse_list)
    mse_svr_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## XGB+RF"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

# xgboost_dict = {}
# rf_dict = {}
m_dict={}
mse_1_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold 객체 생성
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # 종속 변수 및 독립 변수 분리
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # 각 fold에 대해 모델 학습 및 평가
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # XGBoost 모델 학습
        xgboost_pred=xgboost_dict[label].predict(X_test)

        # RandomForest 모델 학습
        rf_pred=rf_dict[label].predict(X_test)

        # 앙상블을 위한 예측값 결합
        y_pred = (xgboost_pred + rf_pred) / 2

        # 평가: MSE 계산
        mse = mean_squared_error(y_test, y_pred)
        mse_list.append(mse)

    # 평균 MSE 계산
    avg_mse = sum(mse_list) / len(mse_list)
    mse_1_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## XGB+SVR"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
import xgboost as xgb

# xgboost_dict = {}
# svr_dict = {}
mse_2_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold 객체 생성
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # 종속 변수 및 독립 변수 분리
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # 각 fold에 대해 모델 학습 및 평가
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # XGBoost 모델 학습
        xgboost_pred=xgboost_dict[label].predict(X_test)

        # SVR 모델 학습
        svr_pred=svr_dict[label].predict(X_test)

        # 앙상블을 위한 예측값 결합
        y_pred = (xgboost_pred + svr_pred) / 2

        # 평가: MSE 계산
        mse = mean_squared_error(y_test, y_pred)
        mse_list.append(mse)

    # 평균 MSE 계산
    avg_mse = sum(mse_list) / len(mse_list)
    mse_2_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## SVR+RF"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
import xgboost as xgb

# rf_dict = {}
# svr_dict = {}
mse_3_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold 객체 생성
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # 종속 변수 및 독립 변수 분리
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # 각 fold에 대해 모델 학습 및 평가
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # RandomForest 모델 학습
        rf_pred=rf_dict[label].predict(X_test)

        # SVR 모델 학습
        svr_pred=svr_dict[label].predict(X_test)

        # 앙상블을 위한 예측값 결합
        y_pred = (rf_pred + svr_pred) / 2

        # 평가: MSE 계산
        mse = mean_squared_error(y_test, y_pred)
        mse_list.append(mse)

    # 평균 MSE 계산
    avg_mse = sum(mse_list) / len(mse_list)
    mse_3_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## XGB+SVR+RF"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
import xgboost as xgb

# xgboost_dict = {}
# rf_dict = {}
# svr_dict = {}
mse_4_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold 객체 생성
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # 종속 변수 및 독립 변수 분리
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # 각 fold에 대해 모델 학습 및 평가
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # # XGBoost 모델 학습
        # xgboost = xgb.XGBRegressor()
        # xgboost.fit(X_train, y_train)
        # xgboost_pred = xgboost.predict(X_test)
        xgboost_pred=xgboost_dict[label].predict(X_test)

        # # RandomForest 모델 학습
        # rf = RandomForestRegressor(n_estimators=100, random_state=42)
        # rf.fit(X_train, y_train)
        # rf_pred = rf.predict(X_test)
        rf_pred=rf_dict[label].predict(X_test)

        # # SVR 모델 학습
        # svr_rbf = SVR(kernel='rbf', C=100, gamma=0.5, epsilon=.1)
        # svr_rbf.fit(X_train, y_train)
        # svr_pred = svr_rbf.predict(X_test)
        svr_pred=svr_dict[label].predict(X_test)

        # 앙상블을 위한 예측값 결합
        y_pred = (xgboost_pred+rf_pred + svr_pred) / 3

        # 평가: MSE 계산
        mse = mean_squared_error(y_test, y_pred)
        mse_list.append(mse)

    # 평균 MSE 계산
    avg_mse = sum(mse_list) / len(mse_list)
    mse_4_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""# 시계열 정보를 반영하여 예측"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# 데이터 불러오기
data = pd.read_csv("/content/drive/MyDrive/BME/modified_data2.csv")

from sklearn.model_selection import train_test_split

# 각 환자의 고유한 patient_id 확인
unique_patients = data['patient_id'].unique()

# 각 환자를 train 및 test 세트로 분할
train_patients, test_patients = train_test_split(unique_patients, test_size=0.2, random_state=42)

# train 및 test 세트에 속하는 인덱스 추출
train_idx = data['patient_id'].isin(train_patients)
test_idx = data['patient_id'].isin(test_patients)

# train 및 test 데이터 분할
train_data = data[train_idx]
test_data = data[test_idx]

#print(train_data,test_data)

train_data.isnull().any()

"""## UPDRS1"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd

# 각 환자의 데이터를 따로 처리하여 시계열 형태로 만들기
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_1'을 포함한 데이터셋에서 필요한 피처만 선택
features = ['FIYGGC(UniMod_4)GGNR', 'P36980', 'NVVYTC(UniMod_4)NEGYSLIGNPVAR', 'AGLAASLAGPHSIVGR', 'KYLYEIAR',
            'FNKPFVFLM(UniMod_35)IEQNTK', 'Q9NYU2', 'LLELTGPK', 'C(UniMod_4)FSGQC(UniMod_4)ISK', 'GLGEISAASEFK',
            'VVEESELAR', 'P00748', 'GKRPYQEGTPC(UniMod_4)SQC(UniMod_4)PSGYHC(UniMod_4)K', 'LVYPSC(UniMod_4)EEK',
            'LEEQAQQIR', 'DALSSVQESQVAQQAR', 'LEPGQQEEYYR', 'TATSEYQTFFNPR', 'HYTNPSQDVTVPC(UniMod_4)PVPPPPPC(UniMod_4)C(UniMod_4)HPR',
            'VGGVQSLGGTGALR', 'LGPLVEQGR', 'C(UniMod_4)MC(UniMod_4)PAENPGC(UniMod_4)R', 'SGIEC(UniMod_4)QLWR',
            'GC(UniMod_4)PTEEGC(UniMod_4)GER', 'M(UniMod_35)ADEAGSEADHEGTHSTKR', 'SIVVSPILIPENQR', 'MKYWGVASFLQK',
            'FFLC(UniMod_4)QVAGDAK', 'P06310', 'GNSYFMVEVK', 'GATLALTQVTPQDER', 'EILSVDC(UniMod_4)STNNPSQAK',
            'P17174', 'NILDRQDPPSVVVTSHQAPGEK', 'GLSAEPGWQAK', 'LLPAQLPAEKEVGPPLPQEAVPLQK', 'KTLLSNLEEAKK',
            'ADSGEGDFLAEGGGVR', 'FTILDSQGK', 'C(UniMod_4)LVEKGDVAFVKHQTVPQNTGGK', 'SC(UniMod_4)VGETTESTQC(UniMod_4)EDEELEHLR',
            'LETPDFQLFK', 'ILGPLSYSK', 'P20774', 'KVESELIKPINPR', 'GAAPPKQEFLDIEDP', 'P16070', 'SVPMVPPGIK',
            'DKETC(UniMod_4)FAEEGKK', 'P27169', 'updrs_1']


train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)


# 모델 정의
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, len(features) - 1), return_sequences=True))  # 각 시퀀스의 길이를 None으로 설정
model.add(Dropout(0.2))  # 드롭아웃 레이어를 추가하여 과적합을 방지합니다.
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizer를 사용하여 모델을 컴파일하고, 학습률을 조정합니다.
adam = Adam(learning_rate=0.001)
model.compile(optimizer=adam, loss='mean_squared_error')

# 모델 훈련
for sequence in train_patient_sequences:
    X_train = sequence[:-1, :-1]  # t일 때의 데이터를 입력으로 사용
    y_train = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_train = np.expand_dims(y_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    model.fit(X_train, y_train, epochs=50, batch_size=16)



# 모델 예측
predictions = []
true_values = []

for sequence in test_patient_sequences:
    X_test = sequence[:-1, :-1]  # t일 때의 데이터를 입력으로 사용
    y_test = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # 현재 시퀀스의 마지막 실제값이 NaN이 아니면 true_values에 추가합니다.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# MSE 계산
predicted_values = np.concatenate(predictions)
mse = np.mean(np.square(np.array(true_values) - predicted_values))
print("Mean Squared Error:", mse)

# R^2 스코어 계산
ss_res = np.sum(np.square(true_values - predicted_values))
ss_tot = np.sum(np.square(true_values - np.mean(true_values)))
r2_score = 1 - (ss_res / ss_tot)
print("R^2 Score:", r2_score)

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer

# 각 환자의 데이터를 따로 처리하여 시계열 형태로 만들기
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_1'을 포함한 데이터셋에서 필요한 피처만 선택
features = ['FIYGGC(UniMod_4)GGNR', 'P36980', 'NVVYTC(UniMod_4)NEGYSLIGNPVAR', 'AGLAASLAGPHSIVGR', 'KYLYEIAR',
            'FNKPFVFLM(UniMod_35)IEQNTK', 'Q9NYU2', 'LLELTGPK', 'C(UniMod_4)FSGQC(UniMod_4)ISK', 'GLGEISAASEFK',
            'VVEESELAR', 'P00748', 'GKRPYQEGTPC(UniMod_4)SQC(UniMod_4)PSGYHC(UniMod_4)K', 'LVYPSC(UniMod_4)EEK',
            'LEEQAQQIR', 'DALSSVQESQVAQQAR', 'LEPGQQEEYYR', 'TATSEYQTFFNPR', 'HYTNPSQDVTVPC(UniMod_4)PVPPPPPC(UniMod_4)C(UniMod_4)HPR',
            'VGGVQSLGGTGALR', 'LGPLVEQGR', 'C(UniMod_4)MC(UniMod_4)PAENPGC(UniMod_4)R', 'SGIEC(UniMod_4)QLWR',
            'GC(UniMod_4)PTEEGC(UniMod_4)GER', 'M(UniMod_35)ADEAGSEADHEGTHSTKR', 'SIVVSPILIPENQR', 'MKYWGVASFLQK',
            'FFLC(UniMod_4)QVAGDAK', 'P06310', 'GNSYFMVEVK', 'GATLALTQVTPQDER', 'EILSVDC(UniMod_4)STNNPSQAK',
            'P17174', 'NILDRQDPPSVVVTSHQAPGEK', 'GLSAEPGWQAK', 'LLPAQLPAEKEVGPPLPQEAVPLQK', 'KTLLSNLEEAKK',
            'ADSGEGDFLAEGGGVR', 'FTILDSQGK', 'C(UniMod_4)LVEKGDVAFVKHQTVPQNTGGK', 'SC(UniMod_4)VGETTESTQC(UniMod_4)EDEELEHLR',
            'LETPDFQLFK', 'ILGPLSYSK', 'P20774', 'KVESELIKPINPR', 'GAAPPKQEFLDIEDP', 'P16070', 'SVPMVPPGIK',
            'DKETC(UniMod_4)FAEEGKK', 'P27169', 'updrs_1']

train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)

# 데이터 스케일링 및 NaN 값 대체
scaler = StandardScaler()
imputer = SimpleImputer(strategy='mean')
scaled_train_sequences = [scaler.fit_transform(imputer.fit_transform(seq)) for seq in train_patient_sequences]
scaled_test_sequences = [scaler.transform(imputer.transform(seq)) for seq in test_patient_sequences]

# PCA 적용 (n_components 설정)
min_samples = min([seq.shape[0] for seq in scaled_train_sequences])  # 각 시퀀스의 최소 샘플 수
n_components = min(10, min_samples - 1)  # 최소 샘플 수에 맞추어 n_components를 설정

pca = PCA(n_components=n_components)
pca_train_sequences = [pca.fit_transform(seq) for seq in scaled_train_sequences]
pca_test_sequences = [pca.transform(seq) for seq in scaled_test_sequences]

# LSTM 모델 정의
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, pca.n_components_), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizer를 사용하여 모델을 컴파일하고, 학습률을 조정합니다.
adam = Adam(learning_rate=0.001)
model.compile(optimizer=adam, loss='mean_squared_error')

# 모델 훈련
for sequence in pca_train_sequences:
    X_train = sequence[:-1, :]  # t일 때의 데이터를 입력으로 사용
    y_train = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_train = np.expand_dims(y_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# 모델 예측
predictions = []
true_values = []

for sequence in pca_test_sequences:
    X_test = sequence[:-1, :]  # t일 때의 데이터를 입력으로 사용
    y_test = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # 현재 시퀀스의 마지막 실제값이 NaN이 아니면 true_values에 추가합니다.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error, explained_variance_score, mean_squared_log_error


# MSE (Mean Squared Error)
mse = mean_squared_error(true_values, predicted_values)
print("Mean Squared Error (MSE):", mse)

# MAE (Mean Absolute Error)
mae = mean_absolute_error(true_values, predicted_values)
print("Mean Absolute Error (MAE):", mae)

# MAPE (Mean Absolute Percentage Error)
mape = mean_absolute_percentage_error(true_values, predicted_values)
print("Mean Absolute Percentage Error (MAPE):", mape)

# R² 스코어 (결정 계수, Coefficient of Determination)
r2 = r2_score(true_values, predicted_values)
print("R^2 Score:", r2)

# RMSE (Root Mean Squared Error)
rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)

# Adjusted R² (조정된 결정 계수)
n = len(true_values)
p = 5  # 예제에서는 설명 변수의 수를 1로 가정
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
print("Adjusted R^2:", adjusted_r2)

# MSLE (Mean Squared Logarithmic Error)
msle = mean_squared_log_error(true_values, predicted_values)
print("Mean Squared Logarithmic Error (MSLE):", msle)

# Explained Variance Score (설명된 분산 점수)
explained_variance = explained_variance_score(true_values, predicted_values)
print("Explained Variance Score:", explained_variance)

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error, explained_variance_score

# MSE 계산
predicted_values = np.concatenate(predictions)
mse = np.mean(np.square(np.array(true_values) - predicted_values))
print("Mean Squared Error:", mse)

# 평균 절대 오차 (Mean Absolute Error, MAE)
mae = mean_absolute_error(true_values, predicted_values)
print("Mean Absolute Error (MAE):", mae)

# 평균 절대 백분율 오차 (Mean Absolute Percentage Error, MAPE)
mape = mean_absolute_percentage_error(true_values, predicted_values)
print("Mean Absolute Percentage Error (MAPE):", mape)

# R² 스코어 (결정 계수, Coefficient of Determination)
r2 = r2_score(true_values, predicted_values)
print("R^2 Score:", r2)

# Root Mean Squared Error (RMSE)
rmse = np.sqrt(mean_squared_error(true_values, predicted_values))
print("Root Mean Squared Error (RMSE):", rmse)

# Explained Variance Score
explained_variance = explained_variance_score(true_values, predicted_values)
print("Explained Variance Score:", explained_variance)

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd

# 각 환자의 데이터를 따로 처리하여 시계열 형태로 만들기
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_1'을 포함한 데이터셋에서 필요한 피처만 선택
features = ['FIYGGC(UniMod_4)GGNR', 'P36980', 'NVVYTC(UniMod_4)NEGYSLIGNPVAR', 'AGLAASLAGPHSIVGR', 'KYLYEIAR',
            'FNKPFVFLM(UniMod_35)IEQNTK', 'Q9NYU2', 'LLELTGPK', 'C(UniMod_4)FSGQC(UniMod_4)ISK', 'GLGEISAASEFK',
            'VVEESELAR', 'P00748', 'GKRPYQEGTPC(UniMod_4)SQC(UniMod_4)PSGYHC(UniMod_4)K', 'LVYPSC(UniMod_4)EEK',
            'LEEQAQQIR', 'DALSSVQESQVAQQAR', 'LEPGQQEEYYR', 'TATSEYQTFFNPR', 'HYTNPSQDVTVPC(UniMod_4)PVPPPPPC(UniMod_4)C(UniMod_4)HPR',
            'VGGVQSLGGTGALR', 'LGPLVEQGR', 'C(UniMod_4)MC(UniMod_4)PAENPGC(UniMod_4)R', 'SGIEC(UniMod_4)QLWR',
            'GC(UniMod_4)PTEEGC(UniMod_4)GER', 'M(UniMod_35)ADEAGSEADHEGTHSTKR', 'SIVVSPILIPENQR', 'MKYWGVASFLQK',
            'FFLC(UniMod_4)QVAGDAK', 'P06310', 'GNSYFMVEVK', 'GATLALTQVTPQDER', 'EILSVDC(UniMod_4)STNNPSQAK',
            'P17174', 'NILDRQDPPSVVVTSHQAPGEK', 'GLSAEPGWQAK', 'LLPAQLPAEKEVGPPLPQEAVPLQK', 'KTLLSNLEEAKK',
            'ADSGEGDFLAEGGGVR', 'FTILDSQGK', 'C(UniMod_4)LVEKGDVAFVKHQTVPQNTGGK', 'SC(UniMod_4)VGETTESTQC(UniMod_4)EDEELEHLR',
            'LETPDFQLFK', 'ILGPLSYSK', 'P20774', 'KVESELIKPINPR', 'GAAPPKQEFLDIEDP', 'P16070', 'SVPMVPPGIK',
            'DKETC(UniMod_4)FAEEGKK', 'P27169', 'updrs_1']


train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)

from keras.optimizers import RMSprop

# 모델 정의
model = Sequential()
model.add(LSTM(units=64, input_shape=(None, len(features) - 1), return_sequences=True))  # 각 시퀀스의 길이를 None으로 설정
model.add(Dropout(0.3))  # 드롭아웃 레이어를 추가하여 과적합을 방지합니다.
model.add(LSTM(units=64, return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(units=64))
model.add(Dropout(0.3))
model.add(Dense(units=1))

# RMSprop optimizer를 사용하여 모델을 컴파일하고, 학습률을 조정합니다.
rmsprop = RMSprop(learning_rate=0.0005)
model.compile(optimizer=rmsprop, loss='mean_squared_error')

# 모델 훈련
for sequence in train_patient_sequences:
    X_train = sequence[:-1, :-1]  # t일 때의 데이터를 입력으로 사용
    y_train = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_train = np.expand_dims(y_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    model.fit(X_train, y_train, epochs=50, batch_size=32)

# 모델 예측 및 평가는 이전과 동일하게 진행합니다.


# 모델 예측
predictions = []
true_values = []

for sequence in test_patient_sequences:
    X_test = sequence[:-1, :-1]  # t일 때의 데이터를 입력으로 사용
    y_test = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # 현재 시퀀스의 마지막 실제값이 NaN이 아니면 true_values에 추가합니다.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error, explained_variance_score, mean_squared_log_error


# MSE (Mean Squared Error)
mse = mean_squared_error(true_values, predicted_values)
print("Mean Squared Error (MSE):", mse)

# MAE (Mean Absolute Error)
mae = mean_absolute_error(true_values, predicted_values)
print("Mean Absolute Error (MAE):", mae)

# MAPE (Mean Absolute Percentage Error)
mape = mean_absolute_percentage_error(true_values, predicted_values)
print("Mean Absolute Percentage Error (MAPE):", mape)

# R² 스코어 (결정 계수, Coefficient of Determination)
r2 = r2_score(true_values, predicted_values)
print("R^2 Score:", r2)

# RMSE (Root Mean Squared Error)
rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)

# Adjusted R² (조정된 결정 계수)
n = len(true_values)
p = 1  # 예제에서는 설명 변수의 수를 1로 가정
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
print("Adjusted R^2:", adjusted_r2)

# MSLE (Mean Squared Logarithmic Error)
msle = mean_squared_log_error(true_values, predicted_values)
print("Mean Squared Logarithmic Error (MSLE):", msle)

# Explained Variance Score (설명된 분산 점수)
explained_variance = explained_variance_score(true_values, predicted_values)
print("Explained Variance Score:", explained_variance)

"""## UPDRS2"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd

# 각 환자의 데이터를 따로 처리하여 시계열 형태로 만들기
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_2'을 포함한 데이터셋에서 필요한 피처만 선택
features =['FIYGGC(UniMod_4)GGNR', 'LQDLYSIVR', 'MKYWGVASFLQK', 'P30086', 'C(UniMod_4)LPVTAPENGK', 'ETYGEMADC(UniMod_4)C(UniMod_4)AK', 'LEPGQQEEYYR', 'GLSAEPGWQAK', 'KYFIDFVAR',
           'M(UniMod_35)YLGYEYVTAIR', 'EQLSLLDRFTEDAKR', 'GAQTQTEEEMTR', 'P07998', 'NPDSSTTGPWC(UniMod_4)YTTDPTVR', 'SC(UniMod_4)VGETTESTQC(UniMod_4)EDEELEHLR',
           'SC(UniMod_4)DKTHTC(UniMod_4)PPC(UniMod_4)PAPELLGGPSVFLFPPKPK', 'LVNEVTEFAK', 'LSKELQAAQAR', 'P19827', 'ALANSLAC(UniMod_4)QGK', 'TSAHGNVAEGETKPDPDVTER',
           'HYEGSTVPEK', 'VKDISEVVTPR', 'Q9NYU2', 'THLGEALAPLSK', 'P01591', 'EDC(UniMod_4)NELPPRR', 'AGC(UniMod_4)VAESTAVC(UniMod_4)R', 'AGLAASLAGPHSIVGR', 'P07333',
           'EHVAHLLFLR', 'P01034', 'QVVAGLNFR', 'KVESELIKPINPR', 'KTLLSNLEEAK', 'P36980', 'HYTNPSQDVTVPC(UniMod_4)PVPPPPPC(UniMod_4)C(UniMod_4)HPR', 'GWVTDGFSSLK',
           'VLLDGVQNPR', 'HLDSVLQQLQTEVYR', 'QRQEELC(UniMod_4)LAR', 'IGDQWDKQHDMGHMMR', 'YGLVTYATYPK', 'VDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSK', 'NFPPSQDASGDLYTTSSQLTLPATQC(UniMod_4)LAGK',
           'SSGLVSNAPGVQIR', 'LTASAPGYLAITK', 'KQINDYVEKGTQGK', 'ADSGEGDFLAEGGGVR', 'TLKIENVSYQDKGNYR','updrs_2']

train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)

# 모델 정의
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, len(features) - 1), return_sequences=True))  # 각 시퀀스의 길이를 None으로 설정
model.add(Dropout(0.2))  # 드롭아웃 레이어를 추가하여 과적합을 방지합니다.
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizer를 사용하여 모델을 컴파일하고, 학습률을 조정합니다.
adam = Adam(learning_rate=0.001)
model.compile(optimizer=adam, loss='mean_squared_error')

# 모델 훈련
for sequence in train_patient_sequences:
    X_train = sequence[:-1, :-1]  # t일 때의 데이터를 입력으로 사용
    y_train = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_train = np.expand_dims(y_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    model.fit(X_train, y_train, epochs=50, batch_size=16)


predictions = []
true_values = []

for sequence in test_patient_sequences:
    X_test = sequence[:-1, :-1]  # t일 때의 데이터를 입력으로 사용
    y_test = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # 현재 시퀀스의 마지막 실제값이 NaN이 아니면 true_values에 추가합니다.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# MSE 계산
predicted_values = np.concatenate(predictions)
mse = np.mean(np.square(np.array(true_values) - predicted_values))
print("Mean Squared Error:", mse)

"""## UPDRS3"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd

# 각 환자의 데이터를 따로 처리하여 시계열 형태로 만들기
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_3'을 포함한 데이터셋에서 필요한 피처만 선택
features = ['TPC(UniMod_4)TVSC(UniMod_4)NIPVVSGKEC(UniMod_4)EEIIR', 'QHVVYGPWNLPQSSYSHLTR', 'FIYGGC(UniMod_4)GGNR', 'LLEVPEGR', 'P13521', 'GLPAPIEK',
            'ETYGEMADC(UniMod_4)C(UniMod_4)AK', 'Q6UXD5', 'KLGQSLDC(UniMod_4)NAEVYVVPWEK', 'SDVMYTDWKK', 'LADGGATNQGRVEIFYR', 'MYLGYEYVTAIR',
            'LSYTC(UniMod_4)EGGFR', 'KLVGYLDR', 'VTSIQDWVQK', 'QTHQPPAPNSLIR', 'FM(UniMod_35)ETVAEK', 'KDSGFQM(UniMod_35)NQLR', 'P10643', 'YANC(UniMod_4)HLAR',
            'M(UniMod_35)YLGYEYVTAIR', 'HYEGSTVPEK', 'P25311', 'P08123', 'KMTVTDQVNC(UniMod_4)PK', 'NPDSSTTGPWC(UniMod_4)YTTDPTVR', 'GYPGVQAPEDLEWER',
            'NFPPSQDASGDLYTTSSQLTLPATQC(UniMod_4)LAGK', 'GSPAINVAVHVFR', 'C(UniMod_4)PNPPVQENFDVNKYLGR', 'P01877', 'P04406', 'EGDMLTLFDGDGPSAR', 'VFQEPLFYEAPR',
            'RLGMFNIQHC(UniMod_4)K', 'QFTSSTSYNR', 'EKLQDEDLGFL', 'THLGEALAPLSK', 'YQC(UniMod_4)YC(UniMod_4)YGR', 'DPTFIPAPIQAK', 'VTEIWQEVMQR', 'SDVMYTDWK',
            'IEIPSSVQQVPTIIK', 'YPSLSIHGIEGAFDEPGTK', 'THPHFVIPYR', 'IASFSQNC(UniMod_4)DIYPGKDFVQPPTK', 'RLEGQEEEEDNRDSSMK', 'Q13332', 'TLKIENVSYQDKGNYR', 'C(UniMod_4)FSGQC(UniMod_4)ISK','updrs_3']


train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)

# 모델 정의
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, len(features) - 1), return_sequences=True))  # 각 시퀀스의 길이를 None으로 설정
model.add(Dropout(0.2))  # 드롭아웃 레이어를 추가하여 과적합을 방지합니다.
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizer를 사용하여 모델을 컴파일하고, 학습률을 조정합니다.
adam = Adam(learning_rate=0.001)
model.compile(optimizer=adam, loss='mean_squared_error')

# 모델 훈련
for sequence in train_patient_sequences:
    X_train = sequence[:-1, :-1]  # t일 때의 데이터를 입력으로 사용
    y_train = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_train = np.expand_dims(y_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# 모델 예측
predictions = []
true_values = []

for sequence in test_patient_sequences:
    X_test = sequence[:-1, :-1]  # t일 때의 데이터를 입력으로 사용
    y_test = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # 현재 시퀀스의 마지막 실제값이 NaN이 아니면 true_values에 추가합니다.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# MSE 계산
predicted_values = np.concatenate(predictions)
mse = np.mean(np.square(np.array(true_values) - predicted_values))
print("Mean Squared Error:", mse)

"""## UPDRS4"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd

# 각 환자의 데이터를 따로 처리하여 시계열 형태로 만들기
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_4'을 포함한 데이터셋에서 필요한 피처만 선택
features = ['VNHVTLSQPK', 'SILENLR', 'STGGISVPGPMGPSGPR', 'M(UniMod_35)LTPEHVFIHPGWK', 'DVLLEAC(UniMod_4)C(UniMod_4)ADGHR', 'AQC(UniMod_4)GGGLLGVR',
            'SVPPSASHVAPTETFTYEWTVPK', 'VC(UniMod_4)PFAGILENGAVR', 'HGNVAEGETKPDPDVTER', 'EHVAHLLFLR', 'KLSSWVLLMK', 'LEPGQQEEYYR', 'EKLQDEDLGFL',
            'C(UniMod_4)PFPSRPDNGFVNYPAKPTLYYK', 'LLDNWDSVTSTFSK', 'Q92520', 'TPSAAYLWVGTGASEAEK', 'PPTSAHGNVAEGETKPDPDVTER', 'WSRPQAPITGYR', 'Q99435',
            'SFQTGLFTAAR', 'MMAVAADTLQR', 'SVPMVPPGIK', 'AADDTWEPFASGK', 'P23083', 'Q99683', 'FIYGGC(UniMod_4)GGNR', 'QFTSSTSYNR', 'AYLEEEC(UniMod_4)PATLRK',
            'FVVTDGGITR', 'VPFDAATLHTSTAMAAQHGMDDDGTGQK', 'GC(UniMod_4)PTEEGC(UniMod_4)GER', 'QKVEPLRAELQEGAR', 'RTHLPEVFLSK', 'Q92876', 'AIGAVPLIQGEYMIPC(UniMod_4)EK',
            'VYC(UniMod_4)DMNTENGGWTVIQNR', 'TSAHGNVAEGETKPDPDVTER', 'AGLAASLAGPHSIVGR', 'AGAAAGGPGVSGVC(UniMod_4)VC(UniMod_4)K', 'RLEAGDHPVELLAR', 'EDC(UniMod_4)NELPPRR',
            'GSPSGEVSHPR', 'C(UniMod_4)LAPLEGAR', 'VTGVVLFR', 'GNSYFMVEVK', 'LDEVKEQVAEVR', 'ATEDEGSEQKIPEATNR', 'SVIPSDGPSVAC(UniMod_4)VK', 'C(UniMod_4)VC(UniMod_4)PVSNAMC(UniMod_4)R','updrs_4']


train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)

# 모델 정의
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, len(features) - 1), return_sequences=True))
model.add(Dropout(0.2))  # 드롭아웃 레이어를 추가하여 과적합을 방지합니다.
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

adam = Adam(learning_rate=0.001)
model.compile(optimizer=adam, loss='mean_squared_error')

# 모델 훈련
for sequence in train_patient_sequences:
    X_train = sequence[:-1, :-1]  # t일 때의 데이터를 입력으로 사용
    y_train = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_train = np.expand_dims(y_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# 모델 예측
predictions = []
true_values = []

for sequence in test_patient_sequences:
    X_test = sequence[:-1, :-1]  # t일 때의 데이터를 입력으로 사용
    y_test = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # 현재 시퀀스의 마지막 실제값이 NaN이 아니면 true_values에 추가합니다.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# MSE 계산
predicted_values = np.concatenate(predictions)
mse = np.mean(np.square(np.array(true_values) - predicted_values))
print("Mean Squared Error:", mse)

"""# **찐 PCA**

**UPDRS1: MSE=14**
"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences

# 데이터 불러오기
updrs1 = pd.read_csv("/content/drive/MyDrive/BME/updrs1.csv")

# 각 환자의 고유한 patient_id 확인
unique_patients = updrs1['patient_id'].unique()

# 각 환자를 train 및 test 세트로 분할
train_patients, test_patients = train_test_split(unique_patients, test_size=0.2, random_state=42)

# train 및 test 세트에 속하는 인덱스 추출
train_idx = updrs1['patient_id'].isin(train_patients)
test_idx = updrs1['patient_id'].isin(test_patients)

# train 및 test 데이터 분할
train_data = updrs1[train_idx]
test_data = updrs1[test_idx]

# Extract patient IDs
train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

# Drop non-feature columns and the target column
train_features = train_data.drop(columns=['patient_id','visit_id','visit_month','updrs_1']).columns
test_features = test_data.drop(columns=['patient_id','visit_id','visit_month','updrs_1']).columns

# Extract target columns
train_targets = train_data['updrs_1']
test_targets = test_data['updrs_1']

train_patient_sequences = []
test_patient_sequences = []
train_target_sequences = []
test_target_sequences = []

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][train_features].values
    patient_target = train_data[train_data['patient_id'] == patient_id]['updrs_1'].values
    train_patient_sequences.append(patient_data)
    train_target_sequences.append(patient_target)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][test_features].values
    patient_target = test_data[test_data['patient_id'] == patient_id]['updrs_1'].values
    test_patient_sequences.append(patient_data)
    test_target_sequences.append(patient_target)

# 최대 시퀀스 길이 계산 (중간 차원의 최대 길이)
max_seq_len = max(len(patient) for patient in train_patient_sequences)
max_seq_len_t = max(len(patient) for patient in test_patient_sequences)

print(max_seq_len)
print(max_seq_len_t)

# 각 시퀀스를 동일한 길이로 패딩
data_3d_padded = pad_sequences(train_patient_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testdata_3d_padded = pad_sequences(test_patient_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# 타겟 시퀀스를 동일한 길이로 패딩
target_3d_padded = pad_sequences(train_target_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testtarget_3d_padded = pad_sequences(test_target_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# 3차원 배열로 변환
data_3d_padded = np.array(data_3d_padded)
testdata_3d_padded = np.array(testdata_3d_padded)
target_3d_padded = np.array(target_3d_padded)
testtarget_3d_padded = np.array(testtarget_3d_padded)

print("3D array shape:", data_3d_padded.shape)
print("3D array shape:", testdata_3d_padded.shape)
print("Target 3D array shape:", target_3d_padded.shape)
print("Test Target 3D array shape:", testtarget_3d_padded.shape)

# 3차원 데이터를 2차원으로 변환 (198*max_seq_len, 1196)
num_samples, num_timesteps, num_features = data_3d_padded.shape
num_samplest, num_timestepst, num_featurest = testdata_3d_padded.shape

data_2d = data_3d_padded.reshape(num_samples * num_timesteps, num_features)
testdata_2d = testdata_3d_padded.reshape(num_samplest * num_timestepst, num_featurest)

# 데이터 표준화
scaler = StandardScaler()
data_2d_scaled = scaler.fit_transform(data_2d)
testdata_2d_scaled = scaler.fit_transform(testdata_2d)

# PCA 모델 생성
pca = PCA(n_components=100)

# PCA 모델을 사용하여 데이터를 2차원으로 압축
data_2d_compressed = pca.fit_transform(data_2d_scaled)
testdata_2d_compressed = pca.fit_transform(testdata_2d_scaled)

# 압축된 데이터를 다시 3차원으로 변환 (198, max_seq_len, 100)
pca_3d_sequences = data_2d_compressed.reshape(num_samples, num_timesteps, 100)
tpca_3d_sequences = testdata_2d_compressed.reshape(num_samplest, num_timestepst, 100)

# 타겟 컬럼 추가 (198, max_seq_len, 101)
pca_3d_sequences_with_target = np.concatenate([pca_3d_sequences, target_3d_padded[..., np.newaxis]], axis=-1)
tpca_3d_sequences_with_target = np.concatenate([tpca_3d_sequences, testtarget_3d_padded[..., np.newaxis]], axis=-1)

print("PCA 3D sequences with target shape:", pca_3d_sequences_with_target.shape)
print("Test PCA 3D sequences with target shape:", tpca_3d_sequences_with_target.shape)

# LSTM 모델 정의
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, 101), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizer를 사용하여 모델을 컴파일하고, 학습률을 조정합니다.
adam = Adam(learning_rate=0.01)
model.compile(optimizer=adam, loss='mean_squared_error')

# 모델 훈련
for sequence in pca_3d_sequences_with_target:
    X_train = sequence[:-1, :]  # t일 때의 데이터를 입력으로 사용
    y_train = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_train = np.expand_dims(y_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# 모델 예측
predictions = []
true_values = []

for sequence in tpca_3d_sequences_with_target:
    X_test = sequence[:-1, :]  # t일 때의 데이터를 입력으로 사용
    y_test = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # 현재 시퀀스의 마지막 실제값이 NaN이 아니면 true_values에 추가합니다.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# Flatten the predictions list and true_values list
predicted_values = np.concatenate(predictions)
true_values = np.array(true_values)

# Evaluation metrics
mse = mean_squared_error(true_values, predicted_values)
print("Mean Squared Error (MSE):", mse)

mae = mean_absolute_error(true_values, predicted_values)
print("Mean Absolute Error (MAE):", mae)

mape = mean_absolute_percentage_error(true_values, predicted_values)
print("Mean Absolute Percentage Error (MAPE):", mape)

r2 = r2_score(true_values, predicted_values)
print("R^2 Score:", r2)

rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)

"""**UPDRS2 : MSE=20**"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences

# 데이터 불러오기
updrs2 = pd.read_csv("/content/drive/MyDrive/BME/updrs2.csv")

# 각 환자의 고유한 patient_id 확인
unique_patients = updrs2['patient_id'].unique()

# 각 환자를 train 및 test 세트로 분할
train_patients, test_patients = train_test_split(unique_patients, test_size=0.2, random_state=42)

# train 및 test 세트에 속하는 인덱스 추출
train_idx = updrs2['patient_id'].isin(train_patients)
test_idx = updrs2['patient_id'].isin(test_patients)

# train 및 test 데이터 분할
train_data = updrs2[train_idx]
test_data = updrs2[test_idx]

# Extract patient IDs
train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

# Drop non-feature columns and the target column
train_features = train_data.drop(columns=['patient_id','visit_id','visit_month','updrs_2']).columns
test_features = test_data.drop(columns=['patient_id','visit_id','visit_month','updrs_2']).columns

# Extract target columns
train_targets = train_data['updrs_2']
test_targets = test_data['updrs_2']

train_patient_sequences = []
test_patient_sequences = []
train_target_sequences = []
test_target_sequences = []

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][train_features].values
    patient_target = train_data[train_data['patient_id'] == patient_id]['updrs_2'].values
    train_patient_sequences.append(patient_data)
    train_target_sequences.append(patient_target)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][test_features].values
    patient_target = test_data[test_data['patient_id'] == patient_id]['updrs_2'].values
    test_patient_sequences.append(patient_data)
    test_target_sequences.append(patient_target)

# 최대 시퀀스 길이 계산 (중간 차원의 최대 길이)
max_seq_len = max(len(patient) for patient in train_patient_sequences)
max_seq_len_t = max(len(patient) for patient in test_patient_sequences)

print(max_seq_len)
print(max_seq_len_t)

# 각 시퀀스를 동일한 길이로 패딩
data_3d_padded = pad_sequences(train_patient_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testdata_3d_padded = pad_sequences(test_patient_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# 타겟 시퀀스를 동일한 길이로 패딩
target_3d_padded = pad_sequences(train_target_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testtarget_3d_padded = pad_sequences(test_target_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# 3차원 배열로 변환
data_3d_padded = np.array(data_3d_padded)
testdata_3d_padded = np.array(testdata_3d_padded)
target_3d_padded = np.array(target_3d_padded)
testtarget_3d_padded = np.array(testtarget_3d_padded)

print("3D array shape:", data_3d_padded.shape)
print("3D array shape:", testdata_3d_padded.shape)
print("Target 3D array shape:", target_3d_padded.shape)
print("Test Target 3D array shape:", testtarget_3d_padded.shape)

# 3차원 데이터를 2차원으로 변환 (198*max_seq_len, 1196)
num_samples, num_timesteps, num_features = data_3d_padded.shape
num_samplest, num_timestepst, num_featurest = testdata_3d_padded.shape

data_2d = data_3d_padded.reshape(num_samples * num_timesteps, num_features)
testdata_2d = testdata_3d_padded.reshape(num_samplest * num_timestepst, num_featurest)

# 데이터 표준화
scaler = StandardScaler()
data_2d_scaled = scaler.fit_transform(data_2d)
testdata_2d_scaled = scaler.fit_transform(testdata_2d)

# PCA 모델 생성
pca = PCA(n_components=100)

# PCA 모델을 사용하여 데이터를 2차원으로 압축
data_2d_compressed = pca.fit_transform(data_2d_scaled)
testdata_2d_compressed = pca.fit_transform(testdata_2d_scaled)

# 압축된 데이터를 다시 3차원으로 변환 (198, max_seq_len, 100)
pca_3d_sequences = data_2d_compressed.reshape(num_samples, num_timesteps, 100)
tpca_3d_sequences = testdata_2d_compressed.reshape(num_samplest, num_timestepst, 100)

# 타겟 컬럼 추가 (198, max_seq_len, 101)
pca_3d_sequences_with_target = np.concatenate([pca_3d_sequences, target_3d_padded[..., np.newaxis]], axis=-1)
tpca_3d_sequences_with_target = np.concatenate([tpca_3d_sequences, testtarget_3d_padded[..., np.newaxis]], axis=-1)

print("PCA 3D sequences with target shape:", pca_3d_sequences_with_target.shape)
print("Test PCA 3D sequences with target shape:", tpca_3d_sequences_with_target.shape)

# LSTM 모델 정의
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, 101), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizer를 사용하여 모델을 컴파일하고, 학습률을 조정합니다.
adam = Adam(learning_rate=0.01)
model.compile(optimizer=adam, loss='mean_squared_error')

# 모델 훈련
for sequence in pca_3d_sequences_with_target:
    X_train = sequence[:-1, :]  # t일 때의 데이터를 입력으로 사용
    y_train = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_train = np.expand_dims(y_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# 모델 예측
predictions = []
true_values = []

for sequence in tpca_3d_sequences_with_target:
    X_test = sequence[:-1, :]  # t일 때의 데이터를 입력으로 사용
    y_test = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # 현재 시퀀스의 마지막 실제값이 NaN이 아니면 true_values에 추가합니다.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# Flatten the predictions list and true_values list
predicted_values = np.concatenate(predictions)
true_values = np.array(true_values)

# Evaluation metrics
mse = mean_squared_error(true_values, predicted_values)
print("Mean Squared Error (MSE):", mse)

mae = mean_absolute_error(true_values, predicted_values)
print("Mean Absolute Error (MAE):", mae)

mape = mean_absolute_percentage_error(true_values, predicted_values)
print("Mean Absolute Percentage Error (MAPE):", mape)

r2 = r2_score(true_values, predicted_values)
print("R^2 Score:", r2)

rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)

"""**UPDRS3: MSE=143**"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences

# 데이터 불러오기
updrs3 = pd.read_csv("/content/drive/MyDrive/BME/updrs3.csv")

# 각 환자의 고유한 patient_id 확인
unique_patients = updrs3['patient_id'].unique()

# 각 환자를 train 및 test 세트로 분할
train_patients, test_patients = train_test_split(unique_patients, test_size=0.2, random_state=42)

# train 및 test 세트에 속하는 인덱스 추출
train_idx = updrs3['patient_id'].isin(train_patients)
test_idx = updrs3['patient_id'].isin(test_patients)

# train 및 test 데이터 분할
train_data = updrs3[train_idx]
test_data = updrs3[test_idx]

# Extract patient IDs
train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

# Drop non-feature columns and the target column
train_features = train_data.drop(columns=['patient_id','visit_id','visit_month','updrs_3']).columns
test_features = test_data.drop(columns=['patient_id','visit_id','visit_month','updrs_3']).columns

# Extract target columns
train_targets = train_data['updrs_3']
test_targets = test_data['updrs_3']

train_patient_sequences = []
test_patient_sequences = []
train_target_sequences = []
test_target_sequences = []

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][train_features].values
    patient_target = train_data[train_data['patient_id'] == patient_id]['updrs_3'].values
    train_patient_sequences.append(patient_data)
    train_target_sequences.append(patient_target)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][test_features].values
    patient_target = test_data[test_data['patient_id'] == patient_id]['updrs_3'].values
    test_patient_sequences.append(patient_data)
    test_target_sequences.append(patient_target)

# 최대 시퀀스 길이 계산 (중간 차원의 최대 길이)
max_seq_len = max(len(patient) for patient in train_patient_sequences)
max_seq_len_t = max(len(patient) for patient in test_patient_sequences)

print(max_seq_len)
print(max_seq_len_t)

# 각 시퀀스를 동일한 길이로 패딩
data_3d_padded = pad_sequences(train_patient_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testdata_3d_padded = pad_sequences(test_patient_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# 타겟 시퀀스를 동일한 길이로 패딩
target_3d_padded = pad_sequences(train_target_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testtarget_3d_padded = pad_sequences(test_target_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# 3차원 배열로 변환
data_3d_padded = np.array(data_3d_padded)
testdata_3d_padded = np.array(testdata_3d_padded)
target_3d_padded = np.array(target_3d_padded)
testtarget_3d_padded = np.array(testtarget_3d_padded)

print("3D array shape:", data_3d_padded.shape)
print("3D array shape:", testdata_3d_padded.shape)
print("Target 3D array shape:", target_3d_padded.shape)
print("Test Target 3D array shape:", testtarget_3d_padded.shape)

# 3차원 데이터를 2차원으로 변환 (198*max_seq_len, 1196)
num_samples, num_timesteps, num_features = data_3d_padded.shape
num_samplest, num_timestepst, num_featurest = testdata_3d_padded.shape

data_2d = data_3d_padded.reshape(num_samples * num_timesteps, num_features)
testdata_2d = testdata_3d_padded.reshape(num_samplest * num_timestepst, num_featurest)

# 데이터 표준화
scaler = StandardScaler()
data_2d_scaled = scaler.fit_transform(data_2d)
testdata_2d_scaled = scaler.fit_transform(testdata_2d)

# PCA 모델 생성
pca = PCA(n_components=100)

# PCA 모델을 사용하여 데이터를 2차원으로 압축
data_2d_compressed = pca.fit_transform(data_2d_scaled)
testdata_2d_compressed = pca.fit_transform(testdata_2d_scaled)

# 압축된 데이터를 다시 3차원으로 변환 (198, max_seq_len, 100)
pca_3d_sequences = data_2d_compressed.reshape(num_samples, num_timesteps, 100)
tpca_3d_sequences = testdata_2d_compressed.reshape(num_samplest, num_timestepst, 100)

# 타겟 컬럼 추가 (198, max_seq_len, 101)
pca_3d_sequences_with_target = np.concatenate([pca_3d_sequences, target_3d_padded[..., np.newaxis]], axis=-1)
tpca_3d_sequences_with_target = np.concatenate([tpca_3d_sequences, testtarget_3d_padded[..., np.newaxis]], axis=-1)

print("PCA 3D sequences with target shape:", pca_3d_sequences_with_target.shape)
print("Test PCA 3D sequences with target shape:", tpca_3d_sequences_with_target.shape)

# LSTM 모델 정의
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, 101), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizer를 사용하여 모델을 컴파일하고, 학습률을 조정합니다.
adam = Adam(learning_rate=0.01)
model.compile(optimizer=adam, loss='mean_squared_error')

# 모델 훈련
for sequence in pca_3d_sequences_with_target:
    X_train = sequence[:-1, :]  # t일 때의 데이터를 입력으로 사용
    y_train = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_train = np.expand_dims(y_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# 모델 예측
predictions = []
true_values = []

for sequence in tpca_3d_sequences_with_target:
    X_test = sequence[:-1, :]  # t일 때의 데이터를 입력으로 사용
    y_test = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # 현재 시퀀스의 마지막 실제값이 NaN이 아니면 true_values에 추가합니다.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# Flatten the predictions list and true_values list
predicted_values = np.concatenate(predictions)
true_values = np.array(true_values)

# Evaluation metrics
mse = mean_squared_error(true_values, predicted_values)
print("Mean Squared Error (MSE):", mse)

mae = mean_absolute_error(true_values, predicted_values)
print("Mean Absolute Error (MAE):", mae)

mape = mean_absolute_percentage_error(true_values, predicted_values)
print("Mean Absolute Percentage Error (MAPE):", mape)

r2 = r2_score(true_values, predicted_values)
print("R^2 Score:", r2)

rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)

"""**UPDRS4: MSE=**"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences

# 데이터 불러오기
updrs4 = pd.read_csv("/content/drive/MyDrive/BME/updrs4.csv")

# 각 환자의 고유한 patient_id 확인
unique_patients = updrs4['patient_id'].unique()

# 각 환자를 train 및 test 세트로 분할
train_patients, test_patients = train_test_split(unique_patients, test_size=0.2, random_state=42)

# train 및 test 세트에 속하는 인덱스 추출
train_idx = updrs4['patient_id'].isin(train_patients)
test_idx = updrs4['patient_id'].isin(test_patients)

# train 및 test 데이터 분할
train_data = updrs4[train_idx]
test_data = updrs4[test_idx]

# Extract patient IDs
train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

# Drop non-feature columns and the target column
train_features = train_data.drop(columns=['patient_id','visit_id','visit_month','updrs_4']).columns
test_features = test_data.drop(columns=['patient_id','visit_id','visit_month','updrs_4']).columns

# Extract target columns
train_targets = train_data['updrs_4']
test_targets = test_data['updrs_4']

train_patient_sequences = []
test_patient_sequences = []
train_target_sequences = []
test_target_sequences = []

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][train_features].values
    patient_target = train_data[train_data['patient_id'] == patient_id]['updrs_4'].values
    train_patient_sequences.append(patient_data)
    train_target_sequences.append(patient_target)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][test_features].values
    patient_target = test_data[test_data['patient_id'] == patient_id]['updrs_4'].values
    test_patient_sequences.append(patient_data)
    test_target_sequences.append(patient_target)

# 최대 시퀀스 길이 계산 (중간 차원의 최대 길이)
max_seq_len = max(len(patient) for patient in train_patient_sequences)
max_seq_len_t = max(len(patient) for patient in test_patient_sequences)

print(max_seq_len)
print(max_seq_len_t)

# 각 시퀀스를 동일한 길이로 패딩
data_3d_padded = pad_sequences(train_patient_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testdata_3d_padded = pad_sequences(test_patient_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# 타겟 시퀀스를 동일한 길이로 패딩
target_3d_padded = pad_sequences(train_target_sequences, maxlen=max_seq_len, dtype='float32', padding='post')
testtarget_3d_padded = pad_sequences(test_target_sequences, maxlen=max_seq_len_t, dtype='float32', padding='post')

# 3차원 배열로 변환
data_3d_padded = np.array(data_3d_padded)
testdata_3d_padded = np.array(testdata_3d_padded)
target_3d_padded = np.array(target_3d_padded)
testtarget_3d_padded = np.array(testtarget_3d_padded)

print("3D array shape:", data_3d_padded.shape)
print("3D array shape:", testdata_3d_padded.shape)
print("Target 3D array shape:", target_3d_padded.shape)
print("Test Target 3D array shape:", testtarget_3d_padded.shape)

# 3차원 데이터를 2차원으로 변환 (198*max_seq_len, 1196)
num_samples, num_timesteps, num_features = data_3d_padded.shape
num_samplest, num_timestepst, num_featurest = testdata_3d_padded.shape

data_2d = data_3d_padded.reshape(num_samples * num_timesteps, num_features)
testdata_2d = testdata_3d_padded.reshape(num_samplest * num_timestepst, num_featurest)

# 데이터 표준화
scaler = StandardScaler()
data_2d_scaled = scaler.fit_transform(data_2d)
testdata_2d_scaled = scaler.fit_transform(testdata_2d)

# PCA 모델 생성
pca = PCA(n_components=100)

# PCA 모델을 사용하여 데이터를 2차원으로 압축
data_2d_compressed = pca.fit_transform(data_2d_scaled)
testdata_2d_compressed = pca.fit_transform(testdata_2d_scaled)

# 압축된 데이터를 다시 3차원으로 변환 (198, max_seq_len, 100)
pca_3d_sequences = data_2d_compressed.reshape(num_samples, num_timesteps, 100)
tpca_3d_sequences = testdata_2d_compressed.reshape(num_samplest, num_timestepst, 100)

# 타겟 컬럼 추가 (198, max_seq_len, 101)
pca_3d_sequences_with_target = np.concatenate([pca_3d_sequences, target_3d_padded[..., np.newaxis]], axis=-1)
tpca_3d_sequences_with_target = np.concatenate([tpca_3d_sequences, testtarget_3d_padded[..., np.newaxis]], axis=-1)

print("PCA 3D sequences with target shape:", pca_3d_sequences_with_target.shape)
print("Test PCA 3D sequences with target shape:", tpca_3d_sequences_with_target.shape)

# LSTM 모델 정의
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, 101), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizer를 사용하여 모델을 컴파일하고, 학습률을 조정합니다.
adam = Adam(learning_rate=0.01)
model.compile(optimizer=adam, loss='mean_squared_error')

# 모델 훈련
for sequence in pca_3d_sequences_with_target:
    X_train = sequence[:-1, :]  # t일 때의 데이터를 입력으로 사용
    y_train = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_train = np.expand_dims(y_train, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# 모델 예측
predictions = []
true_values = []

for sequence in tpca_3d_sequences_with_target:
    X_test = sequence[:-1, :]  # t일 때의 데이터를 입력으로 사용
    y_test = sequence[1:, -1]  # t+1일 때의 updrs 점수를 타겟으로 사용

    # null 값을 가진 행을 스킵합니다.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM 모델 입력을 위해 차원 확장
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # 현재 시퀀스의 마지막 실제값이 NaN이 아니면 true_values에 추가합니다.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# Flatten the predictions list and true_values list
predicted_values = np.concatenate(predictions)
true_values = np.array(true_values)

# Evaluation metrics
mse = mean_squared_error(true_values, predicted_values)
print("Mean Squared Error (MSE):", mse)

mae = mean_absolute_error(true_values, predicted_values)
print("Mean Absolute Error (MAE):", mae)

mape = mean_absolute_percentage_error(true_values, predicted_values)
print("Mean Absolute Percentage Error (MAPE):", mape)

r2 = r2_score(true_values, predicted_values)
print("R^2 Score:", r2)

rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)