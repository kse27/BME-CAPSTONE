# -*- coding: utf-8 -*-
"""BME Capstone

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c1vqEGlqlw-oNHchHQOYDC3zXh1Z6Gmw

# ğŸ§¸ BME Capstone
:predicting MDS-UPDR scores by using protein and peptide levels

ğŸ‘‰ 202100805 ê¹€ì‹œì€, 202102238 ìœ í¬ì§„

# ë°ì´í„° ì „ì²˜ë¦¬
"""

import csv
from collections import defaultdict
import pandas as pd

#Load a dataset into a Pandas DataFrame
train_proteins = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_proteins.csv")
train_peptides = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_peptides.csv")
train_clinical = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_clinical_data.csv")

#Uniprot, peptide ì¢…ë¥˜ ìˆ˜ í™•ì¸

unique_Uniprot = train_proteins['UniProt'].nunique()
print("Unique values in 'Uniprot' column:", unique_Uniprot)

unique_peptide=train_peptides['Peptide'].nunique()
print("Unique valeus in 'Peptide' column:", unique_peptide)

# Function to prepare dataset with all the steps mentioned above:
def prepare_dataset(train_proteins, train_peptides):
    # Step 1: Grouping
    df_protein_grouped = train_proteins.groupby(['visit_id','UniProt'])['NPX'].mean().reset_index()
    df_peptide_grouped = train_peptides.groupby(['visit_id','Peptide'])['PeptideAbundance'].mean().reset_index()

    # Step 2: Pivoting
    df_protein = df_protein_grouped.pivot(index='visit_id',columns = 'UniProt', values = 'NPX').rename_axis(columns=None).reset_index()
    df_peptide = df_peptide_grouped.pivot(index='visit_id',columns = 'Peptide', values = 'PeptideAbundance').rename_axis(columns=None).reset_index()

    # Step 3: Merging
    pro_pep_df = df_protein.merge(df_peptide, on = ['visit_id'], how = 'left')

    return pro_pep_df

pro_pep_df = prepare_dataset(train_proteins, train_peptides)
# CSV íŒŒì¼ë¡œ ì €ì¥
pro_pep_df.to_csv('/content/drive/MyDrive/BME/a_data.csv', index=False)
print(pro_pep_df.head())

import pandas as pd
a_data = pd.read_csv("/content/drive/MyDrive/BME/a_data.csv")
train_clinical = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_clinical_data.csv")

# visit_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‘ íŒŒì¼ ë³‘í•© (ì™¸ë¶€ ì¡°ì¸)
merged_df = pd.merge(a_data, train_clinical, on='visit_id', how='outer')

# ë§ˆì§€ë§‰ ì—´ì„ ì œê±° - ì•½ë¬¼ ë³µìš© ì—¬ë¶€
df = merged_df.drop(merged_df.columns[-1], axis=1)

# 'patient id'ì™€ 'visitmonth' ì—´ì„ ì œê±°í•©ë‹ˆë‹¤
df = df.drop(['patient_id', 'visit_month'], axis=1)

# 'visit_id' ì—´ì„ '_'ë¡œ ë¶„í• í•˜ì—¬ 'patient_id'ì™€ 'visit_month'ë¡œ ë‚˜ëˆ„ê¸°
df[['patient_id', 'visit_month']] = df['visit_id'].str.split('_', expand=True)

# 'patient_id'ì™€ 'visit_month' ì—´ì˜ ë°ì´í„° íƒ€ì…ì„ ì ì ˆíˆ ë³€í™˜ (ì˜µì…˜)
df['patient_id'] = df['patient_id'].astype(int)
df['visit_month'] = df['visit_month'].astype(int)

# ì—´ ìˆœì„œ ì¡°ì •í•˜ì—¬ ë§¨ ë’¤ì— ìˆëŠ” ë‘ ê°œì˜ ì—´ì„ ë§¨ ì•ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
columns = df.columns.tolist()
new_order = columns[-2:] + columns[:-2]

# ìƒˆë¡œìš´ ìˆœì„œë¡œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
df = df[new_order]

# patient_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•œ í›„, visit_monthë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤
df_sorted = df.sort_values(by=['patient_id', 'visit_month'])

df_sorted

# CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤

from sklearn.impute import SimpleImputer

# ê° í™˜ìì˜ ê° ì—´ì— ëŒ€í•œ ì¡´ì¬í•˜ëŠ” ê°’ë“¤ì˜ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤
patient_means = df_sorted.groupby('patient_id').mean()

# null ê°’ì„ í•´ë‹¹ í™˜ìì˜ í‰ê·  ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤
imputer = SimpleImputer(strategy='mean')

feature_list = df_sorted.columns[3:-4]

# null ê°’ë§Œ ëŒ€ì²´í•©ë‹ˆë‹¤
for col in feature_list:
    mask = df_sorted[col].isnull()  # null ê°’ì„ í™•ì¸í•©ë‹ˆë‹¤
    if mask.any():  # null ê°’ì„ í¬í•¨í•˜ëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤
        for patient_id, mean_value in patient_means[col].items():
            df_sorted.loc[(df_sorted['patient_id'] == patient_id) & mask, col] = mean_value  # í•´ë‹¹ í™˜ìì˜ null ê°’ë§Œ ëŒ€ì²´í•©ë‹ˆë‹¤

df_sorted

#nullì„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê³ ë„ ë‚¨ì•„ìˆëŠ” nullê°’ì€ 0ìœ¼ë¡œ ëŒ€ì²´
for col in feature_list:# íŠ¹ì • ì—´ì— ìˆëŠ” null ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´ (ì˜ˆë¥¼ ë“¤ì–´, 'protein1' ì—´)
  df_sorted[col].fillna(0, inplace=True)

# ê²°ê³¼ í™•ì¸
print(df_sorted.isnull().sum())

df_sorted.to_csv('/content/drive/MyDrive/BME/modified_data2.csv', index=False)

"""#  ì‹œê³„ì—´ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì§€ ì•Šê³  ì˜ˆì¸¡

## XGBoost
"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

xgboost_dict = {}
mse_3_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold ê°ì²´ ìƒì„±
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # ì¢…ì† ë³€ìˆ˜ ë° ë…ë¦½ ë³€ìˆ˜ ë¶„ë¦¬
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # ê° foldì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # XGBoost ëª¨ë¸ í•™ìŠµ
        xgboost = xgb.XGBRegressor()
        xgboost.fit(X_train, y_train)
        xgboost_pred = xgboost.predict(X_test)

        # ëª¨ë¸ ì €ì¥
        xgboost_dict[label]=xgboost

        # í‰ê°€: MSE ê³„ì‚°
        mse = mean_squared_error(y_test, xgboost_pred)
        mse_list.append(mse)

    # í‰ê·  MSE ê³„ì‚°
    avg_mse = sum(mse_list) / len(mse_list)
    mse_3_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

import matplotlib.pyplot as plt
import seaborn as sns

for label,model in xgboost_dict.items():

# íŠ¹ì§• ì¤‘ìš”ë„ í™•ì¸
  feature_importance = model.feature_importances_

# íŠ¹ì§• ì¤‘ìš”ë„ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
  feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})

# ì¤‘ìš”ë„ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
  feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# ì¤‘ìš”ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ìƒìœ„ 10ê°œ íŠ¹ì§• ì„ íƒ
  top_features = feature_importance_df.head(50)

# ìƒìœ„ 10ê°œ íŠ¹ì§• ì‹œê°í™”
  plt.figure(figsize=(10, 6))
  sns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')
  plt.title(f'Top 10 Features by Importance {label}')
  plt.xlabel('Importance')
  plt.ylabel('Feature')
  plt.show()

"""## RF"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor

rf_dict = {}
mse_rf_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold ê°ì²´ ìƒì„±
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # ì¢…ì† ë³€ìˆ˜ ë° ë…ë¦½ ë³€ìˆ˜ ë¶„ë¦¬
    X = data.drop(['visit_id','visit_month','patient_id','updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # ê° foldì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # RandomForest ëª¨ë¸ í•™ìŠµ
        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        rf.fit(X_train, y_train)
        rf_pred = rf.predict(X_test)

        # ëª¨ë¸ ì €ì¥
        rf_dict[label]= rf

        # í‰ê°€: MSE ê³„ì‚°
        mse = mean_squared_error(y_test, rf_pred)
        mse_list.append(mse)

    # í‰ê·  MSE ê³„ì‚°
    avg_mse = sum(mse_list) / len(mse_list)
    mse_rf_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## SVR"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR

svr_dict={}
mse_svr_dict={}

target = ['updrs_1','updrs_2','updrs_3','updrs_4']

# KFold ê°ì²´ ìƒì„±
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # ì¢…ì† ë³€ìˆ˜ ë° ë…ë¦½ ë³€ìˆ˜ ë¶„ë¦¬
    X = data.drop(['visit_id','visit_month','patient_id','updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # ê° foldì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # SVR ëª¨ë¸ í•™ìŠµ
        svr_rbf = SVR(kernel='rbf', C=100, gamma=0.5, epsilon=.1)
        svr_rbf.fit(X_train, y_train)
        svr_pred=svr_rbf.predict(X_test)

        # ëª¨ë¸ ì €ì¥
        svr_dict[label]=svr_rbf

        # í‰ê°€: MSE ê³„ì‚°
        mse = mean_squared_error(y_test, svr_pred)
        mse_list.append(mse)

    # í‰ê·  MSE ê³„ì‚°
    avg_mse = sum(mse_list) / len(mse_list)
    mse_3_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## XGB+RF"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

# xgboost_dict = {}
# rf_dict = {}
m_dict={}
mse_3_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold ê°ì²´ ìƒì„±
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # ì¢…ì† ë³€ìˆ˜ ë° ë…ë¦½ ë³€ìˆ˜ ë¶„ë¦¬
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # ê° foldì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # # XGBoost ëª¨ë¸ í•™ìŠµ
        # xgboost = xgb.XGBRegressor()
        # xgboost.fit(X_train, y_train)
        # xgboost_pred = xgboost.predict(X_test)
        xgboost_pred=xgboost_dict[label].predict(X_test)

        # # RandomForest ëª¨ë¸ í•™ìŠµ
        # rf = RandomForestRegressor(n_estimators=100, random_state=42)
        # rf.fit(X_train, y_train)
        # rf_pred = rf.predict(X_test)
        rf_pred=rf_dict[label].predict(X_test)

        # ì•™ìƒë¸”ì„ ìœ„í•œ ì˜ˆì¸¡ê°’ ê²°í•©
        y_pred = (xgboost_pred + rf_pred) / 2

        # í‰ê°€: MSE ê³„ì‚°
        mse = mean_squared_error(y_test, y_pred)
        mse_list.append(mse)

    # í‰ê·  MSE ê³„ì‚°
    avg_mse = sum(mse_list) / len(mse_list)
    mse_3_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## XGB+SVR"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
import xgboost as xgb

# xgboost_dict = {}
# svr_dict = {}
mse_3_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold ê°ì²´ ìƒì„±
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # ì¢…ì† ë³€ìˆ˜ ë° ë…ë¦½ ë³€ìˆ˜ ë¶„ë¦¬
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # ê° foldì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # # XGBoost ëª¨ë¸ í•™ìŠµ
        # xgboost = xgb.XGBRegressor()
        # xgboost.fit(X_train, y_train)
        # xgboost_pred = xgboost.predict(X_test)
        xgboost_pred=xgboost_dict[label].predict(X_test)

        # # SVR ëª¨ë¸ í•™ìŠµ
        # svr_rbf = SVR(kernel='rbf', C=100, gamma=0.5, epsilon=.1)
        # svr_rbf.fit(X_train, y_train)
        # svr_pred = svr_rbf.predict(X_test)
        svr_pred=svr_dict[label].predict(X_test)

        # ì•™ìƒë¸”ì„ ìœ„í•œ ì˜ˆì¸¡ê°’ ê²°í•©
        y_pred = (xgboost_pred + svr_pred) / 2

        # í‰ê°€: MSE ê³„ì‚°
        mse = mean_squared_error(y_test, y_pred)
        mse_list.append(mse)

    # í‰ê·  MSE ê³„ì‚°
    avg_mse = sum(mse_list) / len(mse_list)
    mse_3_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## SVR+RF"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
import xgboost as xgb

# rf_dict = {}
# svr_dict = {}
mse_3_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold ê°ì²´ ìƒì„±
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # ì¢…ì† ë³€ìˆ˜ ë° ë…ë¦½ ë³€ìˆ˜ ë¶„ë¦¬
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # ê° foldì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # # RandomForest ëª¨ë¸ í•™ìŠµ
        # rf = RandomForestRegressor(n_estimators=100, random_state=42)
        # rf.fit(X_train, y_train)
        # rf_pred = rf.predict(X_test)
        rf_pred=rf_dict[label].predict(X_test)

        # # SVR ëª¨ë¸ í•™ìŠµ
        # svr_rbf = SVR(kernel='rbf', C=100, gamma=0.5, epsilon=.1)
        # svr_rbf.fit(X_train, y_train)
        # svr_pred = svr_rbf.predict(X_test)
        svr_pred=svr_dict[label].predict(X_test)

        # ì•™ìƒë¸”ì„ ìœ„í•œ ì˜ˆì¸¡ê°’ ê²°í•©
        y_pred = (rf_pred + svr_pred) / 2

        # í‰ê°€: MSE ê³„ì‚°
        mse = mean_squared_error(y_test, y_pred)
        mse_list.append(mse)

    # í‰ê·  MSE ê³„ì‚°
    avg_mse = sum(mse_list) / len(mse_list)
    mse_3_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""## XGB+SVR+RF"""

import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
import xgboost as xgb

# xgboost_dict = {}
# rf_dict = {}
# svr_dict = {}
mse_3_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold ê°ì²´ ìƒì„±
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # ì¢…ì† ë³€ìˆ˜ ë° ë…ë¦½ ë³€ìˆ˜ ë¶„ë¦¬
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # ê° foldì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    mse_list = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # # XGBoost ëª¨ë¸ í•™ìŠµ
        # xgboost = xgb.XGBRegressor()
        # xgboost.fit(X_train, y_train)
        # xgboost_pred = xgboost.predict(X_test)
        xgboost_pred=xgboost_dict[label].predict(X_test)

        # # RandomForest ëª¨ë¸ í•™ìŠµ
        # rf = RandomForestRegressor(n_estimators=100, random_state=42)
        # rf.fit(X_train, y_train)
        # rf_pred = rf.predict(X_test)
        rf_pred=rf_dict[label].predict(X_test)

        # # SVR ëª¨ë¸ í•™ìŠµ
        # svr_rbf = SVR(kernel='rbf', C=100, gamma=0.5, epsilon=.1)
        # svr_rbf.fit(X_train, y_train)
        # svr_pred = svr_rbf.predict(X_test)
        svr_pred=svr_dict[label].predict(X_test)

        # ì•™ìƒë¸”ì„ ìœ„í•œ ì˜ˆì¸¡ê°’ ê²°í•©
        y_pred = (xgboost_pred+rf_pred + svr_pred) / 3

        # í‰ê°€: MSE ê³„ì‚°
        mse = mean_squared_error(y_test, y_pred)
        mse_list.append(mse)

    # í‰ê·  MSE ê³„ì‚°
    avg_mse = sum(mse_list) / len(mse_list)
    mse_3_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

"""# ì‹œê³„ì—´ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ ì˜ˆì¸¡"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
data = pd.read_csv("/content/drive/MyDrive/BME/modified_data2.csv")  # ë°ì´í„° íŒŒì¼ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.

from sklearn.model_selection import train_test_split

# ê° í™˜ìì˜ ê³ ìœ í•œ patient_id í™•ì¸
unique_patients = data['patient_id'].unique()

# ê° í™˜ìë¥¼ train ë° test ì„¸íŠ¸ë¡œ ë¶„í• 
train_patients, test_patients = train_test_split(unique_patients, test_size=0.2, random_state=42)

# train ë° test ì„¸íŠ¸ì— ì†í•˜ëŠ” ì¸ë±ìŠ¤ ì¶”ì¶œ
train_idx = data['patient_id'].isin(train_patients)
test_idx = data['patient_id'].isin(test_patients)


# train ë° test ë°ì´í„° ë¶„í• 
train_data = data[train_idx]
test_data = data[test_idx]

#print(train_data,test_data)

train_data.isnull().any()

"""## UPDRS1"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd

# ê° í™˜ìì˜ ë°ì´í„°ë¥¼ ë”°ë¡œ ì²˜ë¦¬í•˜ì—¬ ì‹œê³„ì—´ í˜•íƒœë¡œ ë§Œë“¤ê¸°
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_1'ì„ í¬í•¨í•œ ë°ì´í„°ì…‹ì—ì„œ í•„ìš”í•œ í”¼ì²˜ë§Œ ì„ íƒ
features = ['FIYGGC(UniMod_4)GGNR', 'P36980', 'NVVYTC(UniMod_4)NEGYSLIGNPVAR', 'AGLAASLAGPHSIVGR', 'KYLYEIAR',
            'FNKPFVFLM(UniMod_35)IEQNTK', 'Q9NYU2', 'LLELTGPK', 'C(UniMod_4)FSGQC(UniMod_4)ISK', 'GLGEISAASEFK',
            'VVEESELAR', 'P00748', 'GKRPYQEGTPC(UniMod_4)SQC(UniMod_4)PSGYHC(UniMod_4)K', 'LVYPSC(UniMod_4)EEK',
            'LEEQAQQIR', 'DALSSVQESQVAQQAR', 'LEPGQQEEYYR', 'TATSEYQTFFNPR', 'HYTNPSQDVTVPC(UniMod_4)PVPPPPPC(UniMod_4)C(UniMod_4)HPR',
            'VGGVQSLGGTGALR', 'LGPLVEQGR', 'C(UniMod_4)MC(UniMod_4)PAENPGC(UniMod_4)R', 'SGIEC(UniMod_4)QLWR',
            'GC(UniMod_4)PTEEGC(UniMod_4)GER', 'M(UniMod_35)ADEAGSEADHEGTHSTKR', 'SIVVSPILIPENQR', 'MKYWGVASFLQK',
            'FFLC(UniMod_4)QVAGDAK', 'P06310', 'GNSYFMVEVK', 'GATLALTQVTPQDER', 'EILSVDC(UniMod_4)STNNPSQAK',
            'P17174', 'NILDRQDPPSVVVTSHQAPGEK', 'GLSAEPGWQAK', 'LLPAQLPAEKEVGPPLPQEAVPLQK', 'KTLLSNLEEAKK',
            'ADSGEGDFLAEGGGVR', 'FTILDSQGK', 'C(UniMod_4)LVEKGDVAFVKHQTVPQNTGGK', 'SC(UniMod_4)VGETTESTQC(UniMod_4)EDEELEHLR',
            'LETPDFQLFK', 'ILGPLSYSK', 'P20774', 'KVESELIKPINPR', 'GAAPPKQEFLDIEDP', 'P16070', 'SVPMVPPGIK',
            'DKETC(UniMod_4)FAEEGKK', 'P27169', 'updrs_1']

# train_data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
# test_data = pd.read_csv('/path/to/test_data.csv')  # í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ ì„¤ì •

train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)

# ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, len(features) - 1), return_sequences=True))  # ê° ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ Noneìœ¼ë¡œ ì„¤ì •
model.add(Dropout(0.2))  # ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³ , í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤.
adam = Adam(learning_rate=0.001)  # ì›í•˜ëŠ” í•™ìŠµë¥ ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
model.compile(optimizer=adam, loss='mean_squared_error')

# ëª¨ë¸ í›ˆë ¨
for sequence in train_patient_sequences:
    X_train = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_train = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_train = np.expand_dims(y_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# ëª¨ë¸ ì˜ˆì¸¡
predictions = []
true_values = []

for sequence in test_patient_sequences:
    X_test = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_test = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # í˜„ì¬ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œê°’ì´ NaNì´ ì•„ë‹ˆë©´ true_valuesì— ì¶”ê°€í•©ë‹ˆë‹¤.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# MSE ê³„ì‚°
predicted_values = np.concatenate(predictions)
mse = np.mean(np.square(np.array(true_values) - predicted_values))
print("Mean Squared Error:", mse)

"""## UPDRS2"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd

# ê° í™˜ìì˜ ë°ì´í„°ë¥¼ ë”°ë¡œ ì²˜ë¦¬í•˜ì—¬ ì‹œê³„ì—´ í˜•íƒœë¡œ ë§Œë“¤ê¸°
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_2'ì„ í¬í•¨í•œ ë°ì´í„°ì…‹ì—ì„œ í•„ìš”í•œ í”¼ì²˜ë§Œ ì„ íƒ
features =['FIYGGC(UniMod_4)GGNR', 'LQDLYSIVR', 'MKYWGVASFLQK', 'P30086', 'C(UniMod_4)LPVTAPENGK', 'ETYGEMADC(UniMod_4)C(UniMod_4)AK', 'LEPGQQEEYYR', 'GLSAEPGWQAK', 'KYFIDFVAR',
           'M(UniMod_35)YLGYEYVTAIR', 'EQLSLLDRFTEDAKR', 'GAQTQTEEEMTR', 'P07998', 'NPDSSTTGPWC(UniMod_4)YTTDPTVR', 'SC(UniMod_4)VGETTESTQC(UniMod_4)EDEELEHLR',
           'SC(UniMod_4)DKTHTC(UniMod_4)PPC(UniMod_4)PAPELLGGPSVFLFPPKPK', 'LVNEVTEFAK', 'LSKELQAAQAR', 'P19827', 'ALANSLAC(UniMod_4)QGK', 'TSAHGNVAEGETKPDPDVTER',
           'HYEGSTVPEK', 'VKDISEVVTPR', 'Q9NYU2', 'THLGEALAPLSK', 'P01591', 'EDC(UniMod_4)NELPPRR', 'AGC(UniMod_4)VAESTAVC(UniMod_4)R', 'AGLAASLAGPHSIVGR', 'P07333',
           'EHVAHLLFLR', 'P01034', 'QVVAGLNFR', 'KVESELIKPINPR', 'KTLLSNLEEAK', 'P36980', 'HYTNPSQDVTVPC(UniMod_4)PVPPPPPC(UniMod_4)C(UniMod_4)HPR', 'GWVTDGFSSLK',
           'VLLDGVQNPR', 'HLDSVLQQLQTEVYR', 'QRQEELC(UniMod_4)LAR', 'IGDQWDKQHDMGHMMR', 'YGLVTYATYPK', 'VDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSK', 'NFPPSQDASGDLYTTSSQLTLPATQC(UniMod_4)LAGK',
           'SSGLVSNAPGVQIR', 'LTASAPGYLAITK', 'KQINDYVEKGTQGK', 'ADSGEGDFLAEGGGVR', 'TLKIENVSYQDKGNYR','updrs_2']

train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)

# ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, len(features) - 1), return_sequences=True))  # ê° ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ Noneìœ¼ë¡œ ì„¤ì •
model.add(Dropout(0.2))  # ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³ , í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤.
adam = Adam(learning_rate=0.001)  # ì›í•˜ëŠ” í•™ìŠµë¥ ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
model.compile(optimizer=adam, loss='mean_squared_error')

# ëª¨ë¸ í›ˆë ¨
for sequence in train_patient_sequences:
    X_train = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_train = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_train = np.expand_dims(y_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# ëª¨ë¸ ì˜ˆì¸¡
predictions = []
true_values = []

for sequence in test_patient_sequences:
    X_test = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_test = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # í˜„ì¬ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œê°’ì´ NaNì´ ì•„ë‹ˆë©´ true_valuesì— ì¶”ê°€í•©ë‹ˆë‹¤.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# MSE ê³„ì‚°
predicted_values = np.concatenate(predictions)
mse = np.mean(np.square(np.array(true_values) - predicted_values))
print("Mean Squared Error:", mse)

"""## UPDRS3"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd

# ê° í™˜ìì˜ ë°ì´í„°ë¥¼ ë”°ë¡œ ì²˜ë¦¬í•˜ì—¬ ì‹œê³„ì—´ í˜•íƒœë¡œ ë§Œë“¤ê¸°
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_3'ì„ í¬í•¨í•œ ë°ì´í„°ì…‹ì—ì„œ í•„ìš”í•œ í”¼ì²˜ë§Œ ì„ íƒ
features = ['TPC(UniMod_4)TVSC(UniMod_4)NIPVVSGKEC(UniMod_4)EEIIR', 'QHVVYGPWNLPQSSYSHLTR', 'FIYGGC(UniMod_4)GGNR', 'LLEVPEGR', 'P13521', 'GLPAPIEK',
            'ETYGEMADC(UniMod_4)C(UniMod_4)AK', 'Q6UXD5', 'KLGQSLDC(UniMod_4)NAEVYVVPWEK', 'SDVMYTDWKK', 'LADGGATNQGRVEIFYR', 'MYLGYEYVTAIR',
            'LSYTC(UniMod_4)EGGFR', 'KLVGYLDR', 'VTSIQDWVQK', 'QTHQPPAPNSLIR', 'FM(UniMod_35)ETVAEK', 'KDSGFQM(UniMod_35)NQLR', 'P10643', 'YANC(UniMod_4)HLAR',
            'M(UniMod_35)YLGYEYVTAIR', 'HYEGSTVPEK', 'P25311', 'P08123', 'KMTVTDQVNC(UniMod_4)PK', 'NPDSSTTGPWC(UniMod_4)YTTDPTVR', 'GYPGVQAPEDLEWER',
            'NFPPSQDASGDLYTTSSQLTLPATQC(UniMod_4)LAGK', 'GSPAINVAVHVFR', 'C(UniMod_4)PNPPVQENFDVNKYLGR', 'P01877', 'P04406', 'EGDMLTLFDGDGPSAR', 'VFQEPLFYEAPR',
            'RLGMFNIQHC(UniMod_4)K', 'QFTSSTSYNR', 'EKLQDEDLGFL', 'THLGEALAPLSK', 'YQC(UniMod_4)YC(UniMod_4)YGR', 'DPTFIPAPIQAK', 'VTEIWQEVMQR', 'SDVMYTDWK',
            'IEIPSSVQQVPTIIK', 'YPSLSIHGIEGAFDEPGTK', 'THPHFVIPYR', 'IASFSQNC(UniMod_4)DIYPGKDFVQPPTK', 'RLEGQEEEEDNRDSSMK', 'Q13332', 'TLKIENVSYQDKGNYR', 'C(UniMod_4)FSGQC(UniMod_4)ISK','updrs_3']


train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)

# ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, len(features) - 1), return_sequences=True))  # ê° ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ Noneìœ¼ë¡œ ì„¤ì •
model.add(Dropout(0.2))  # ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Adam optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³ , í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤.
adam = Adam(learning_rate=0.001)  # ì›í•˜ëŠ” í•™ìŠµë¥ ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
model.compile(optimizer=adam, loss='mean_squared_error')

# ëª¨ë¸ í›ˆë ¨
for sequence in train_patient_sequences:
    X_train = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_train = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_train = np.expand_dims(y_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# ëª¨ë¸ ì˜ˆì¸¡
predictions = []
true_values = []

for sequence in test_patient_sequences:
    X_test = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_test = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # í˜„ì¬ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œê°’ì´ NaNì´ ì•„ë‹ˆë©´ true_valuesì— ì¶”ê°€í•©ë‹ˆë‹¤.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# MSE ê³„ì‚°
predicted_values = np.concatenate(predictions)
mse = np.mean(np.square(np.array(true_values) - predicted_values))
print("Mean Squared Error:", mse)

"""## UPDRS4"""

import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam
import pandas as pd

# ê° í™˜ìì˜ ë°ì´í„°ë¥¼ ë”°ë¡œ ì²˜ë¦¬í•˜ì—¬ ì‹œê³„ì—´ í˜•íƒœë¡œ ë§Œë“¤ê¸°
train_patient_sequences = []
test_patient_sequences = []

# 'updrs_4'ì„ í¬í•¨í•œ ë°ì´í„°ì…‹ì—ì„œ í•„ìš”í•œ í”¼ì²˜ë§Œ ì„ íƒ
features = ['VNHVTLSQPK', 'SILENLR', 'STGGISVPGPMGPSGPR', 'M(UniMod_35)LTPEHVFIHPGWK', 'DVLLEAC(UniMod_4)C(UniMod_4)ADGHR', 'AQC(UniMod_4)GGGLLGVR',
            'SVPPSASHVAPTETFTYEWTVPK', 'VC(UniMod_4)PFAGILENGAVR', 'HGNVAEGETKPDPDVTER', 'EHVAHLLFLR', 'KLSSWVLLMK', 'LEPGQQEEYYR', 'EKLQDEDLGFL',
            'C(UniMod_4)PFPSRPDNGFVNYPAKPTLYYK', 'LLDNWDSVTSTFSK', 'Q92520', 'TPSAAYLWVGTGASEAEK', 'PPTSAHGNVAEGETKPDPDVTER', 'WSRPQAPITGYR', 'Q99435',
            'SFQTGLFTAAR', 'MMAVAADTLQR', 'SVPMVPPGIK', 'AADDTWEPFASGK', 'P23083', 'Q99683', 'FIYGGC(UniMod_4)GGNR', 'QFTSSTSYNR', 'AYLEEEC(UniMod_4)PATLRK',
            'FVVTDGGITR', 'VPFDAATLHTSTAMAAQHGMDDDGTGQK', 'GC(UniMod_4)PTEEGC(UniMod_4)GER', 'QKVEPLRAELQEGAR', 'RTHLPEVFLSK', 'Q92876', 'AIGAVPLIQGEYMIPC(UniMod_4)EK',
            'VYC(UniMod_4)DMNTENGGWTVIQNR', 'TSAHGNVAEGETKPDPDVTER', 'AGLAASLAGPHSIVGR', 'AGAAAGGPGVSGVC(UniMod_4)VC(UniMod_4)K', 'RLEAGDHPVELLAR', 'EDC(UniMod_4)NELPPRR',
            'GSPSGEVSHPR', 'C(UniMod_4)LAPLEGAR', 'VTGVVLFR', 'GNSYFMVEVK', 'LDEVKEQVAEVR', 'ATEDEGSEQKIPEATNR', 'SVIPSDGPSVAC(UniMod_4)VK', 'C(UniMod_4)VC(UniMod_4)PVSNAMC(UniMod_4)R','updrs_4']


train_patients = train_data['patient_id'].unique()
test_patients = test_data['patient_id'].unique()

for patient_id in train_patients:
    patient_data = train_data[train_data['patient_id'] == patient_id][features].values
    train_patient_sequences.append(patient_data)

for patient_id in test_patients:
    patient_data = test_data[test_data['patient_id'] == patient_id][features].values
    test_patient_sequences.append(patient_data)

# ëª¨ë¸ ì •ì˜
model = Sequential()
model.add(LSTM(units=50, input_shape=(None, len(features) - 1), return_sequences=True))
model.add(Dropout(0.2))  # ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

adam = Adam(learning_rate=0.001)
model.compile(optimizer=adam, loss='mean_squared_error')

# ëª¨ë¸ í›ˆë ¨
for sequence in train_patient_sequences:
    X_train = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_train = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_train).any():
        continue

    X_train = np.expand_dims(X_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_train = np.expand_dims(y_train, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    model.fit(X_train, y_train, epochs=50, batch_size=16)

# ëª¨ë¸ ì˜ˆì¸¡
predictions = []
true_values = []

for sequence in test_patient_sequences:
    X_test = sequence[:-1, :-1]  # tì¼ ë•Œì˜ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    y_test = sequence[1:, -1]  # t+1ì¼ ë•Œì˜ updrs ì ìˆ˜ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©

    # null ê°’ì„ ê°€ì§„ í–‰ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.
    if np.isnan(y_test).any():
        continue

    X_test = np.expand_dims(X_test, axis=0)  # LSTM ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ ì°¨ì› í™•ì¥
    y_pred = model.predict(X_test)
    predictions.append(y_pred.flatten())

    # í˜„ì¬ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œê°’ì´ NaNì´ ì•„ë‹ˆë©´ true_valuesì— ì¶”ê°€í•©ë‹ˆë‹¤.
    last_value = sequence[-1][-1]
    if not np.isnan(last_value):
        true_values.append(last_value)

# MSE ê³„ì‚°
predicted_values = np.concatenate(predictions)
mse = np.mean(np.square(np.array(true_values) - predicted_values))
print("Mean Squared Error:", mse)
