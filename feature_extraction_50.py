# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zunTw8iGVRSAxkG-D8oCHzUysV2DANGs
"""

from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor

xgboost_dict = {}
rf_dict = {}
mse_1_dict = {}
feature_importance_dict = {}
top_50_features_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold 객체 생성
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # 종속 변수 및 독립 변수 분리
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # 각 fold에 대해 모델 학습 및 평가
    mse_list = []
    xgboost_importances = []
    rf_importances = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # XGBoost 모델 학습
        xgboost_model = xgb.XGBRegressor()
        xgboost_model.fit(X_train, y_train)
        xgboost_pred = xgboost_model.predict(X_test)
        xgboost_importances.append(xgboost_model.feature_importances_)

        # RandomForest 모델 학습
        rf_model = RandomForestRegressor(n_estimators=50, random_state=42)
        rf_model.fit(X_train, y_train)
        rf_pred = rf_model.predict(X_test)
        rf_importances.append(rf_model.feature_importances_)

        # 앙상블을 위한 예측값 결합
        y_pred = (xgboost_pred + rf_pred) / 2

        # 평가: MSE 계산
        mse = mean_squared_error(y_test, y_pred)
        mse_list.append(mse)

    # 평균 MSE 계산
    avg_mse = sum(mse_list) / len(mse_list)
    mse_1_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

    # 평균 feature importance 계산
    avg_xgboost_importances = np.mean(xgboost_importances, axis=0)
    avg_rf_importances = np.mean(rf_importances, axis=0)

    # Combined average feature importance
    combined_importances = (avg_xgboost_importances + avg_rf_importances) / 2
    feature_importance_dict[label] = combined_importances

    # 상위 50개 feature 선택
    feature_names = X.columns
    top_50_indices = np.argsort(combined_importances)[-50:][::-1]
    top_50_features = feature_names[top_50_indices]

    top_50_features_dict[label] = top_50_features

# Display top 50 features for each label
for label in target:
    print(f"Top 50 features for {label}:")
    print(top_50_features_dict[label])
