# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zunTw8iGVRSAxkG-D8oCHzUysV2DANGs
"""

import pandas as pd
from sklearn.impute import SimpleImputer

train_proteins = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_proteins.csv")
train_peptides = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_peptides.csv")
train_clinical = pd.read_csv("/content/drive/MyDrive/BME/amp-parkinsons-disease-progression-prediction/train_clinical_data.csv")

def prepare_dataset(train_proteins, train_peptides):
    # Step 1: Grouping
    df_protein_grouped = train_proteins.groupby(['visit_id','UniProt'])['NPX'].mean().reset_index()
    df_peptide_grouped = train_peptides.groupby(['visit_id','Peptide'])['PeptideAbundance'].mean().reset_index()

    # Step 2: Pivoting
    df_protein = df_protein_grouped.pivot(index='visit_id',columns = 'UniProt', values = 'NPX').rename_axis(columns=None).reset_index()
    df_peptide = df_peptide_grouped.pivot(index='visit_id',columns = 'Peptide', values = 'PeptideAbundance').rename_axis(columns=None).reset_index()

    # Step 3: Merging
    pro_pep_df = df_protein.merge(df_peptide, on = ['visit_id'], how = 'left')

    return pro_pep_df

pro_pep_df = prepare_dataset(train_proteins, train_peptides)
pro_pep_df.to_csv('/content/drive/MyDrive/BME/a_data.csv', index=False) # CSV 파일로 저장
print(pro_pep_df.head())

# visit_id를 기준으로 두 파일 병합 (외부 조인)
merged_df = pd.merge(pro_pep_df, train_clinical, on='visit_id', how='outer')

# 마지막 열을 제거
df = merged_df.drop(merged_df.columns[-1], axis=1)

# 'patient id'와 'visitmonth' 열을 제거합니다
df = df.drop(['patient_id', 'visit_month'], axis=1)

# 'visit_id' 열을 '_'로 분할하여 'patient_id'와 'visit_month'로 나누기
df[['patient_id', 'visit_month']] = df['visit_id'].str.split('_', expand=True)

# 'patient_id'와 'visit_month' 열의 데이터 타입을 적절히 변환
df['patient_id'] = df['patient_id'].astype(int)
df['visit_month'] = df['visit_month'].astype(int)

# 열 순서 조정하여 맨 뒤에 있는 두 개의 열을 맨 앞으로 가져오기
columns = df.columns.tolist()
new_order = columns[-2:] + columns[:-2]

# 새로운 순서로 데이터프레임 생성
df = df[new_order]

# patient_id를 기준으로 오름차순으로 정렬한 후, visit_month를 기준으로 오름차순으로 정렬합니다
df_sorted = df.sort_values(by=['patient_id', 'visit_month'])

df_sorted

# 각 환자의 각 열에 대한 존재하는 값들의 평균을 계산합니다
patient_means = df_sorted.groupby('patient_id').mean()

# null 값을 해당 환자의 평균 값으로 대체합니다
imputer = SimpleImputer(strategy='mean')

feature_list = df_sorted.columns[3:-4]

# null 값만 대체합니다
for col in feature_list:
    mask = df_sorted[col].isnull()  # null 값을 확인합니다
    if mask.any():  # null 값을 포함하는 경우에만 처리합니다
        for patient_id, mean_value in patient_means[col].items():
            df_sorted.loc[(df_sorted['patient_id'] == patient_id) & mask, col] = mean_value  # 해당 환자의 null 값만 대체합니다

df_sorted

#null을 평균값으로 대체하고도 남아있는 null값은 0으로 대체
for col in feature_list:# 특정 열에 있는 null 값을 0으로 대체
  df_sorted[col].fillna(0, inplace=True)

# 결과 확인
print(df_sorted.isnull().sum())

df_sorted.to_csv('/content/drive/MyDrive/BME/preprocessed_data.csv', index=False)

from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor

xgboost_dict = {}
rf_dict = {}
mse_1_dict = {}
feature_importance_dict = {}
top_50_features_dict = {}

target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

# KFold 객체 생성
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for label in target:
    data = pd.read_csv('/content/drive/MyDrive/BME/modified_data2.csv')
    data = data.dropna(subset=[label])

    # 종속 변수 및 독립 변수 분리
    X = data.drop(['visit_id','visit_month','patient_id', 'updrs_1', 'updrs_2', 'updrs_3', 'updrs_4'], axis=1)
    y = data[label]

    # 각 fold에 대해 모델 학습 및 평가
    mse_list = []
    xgboost_importances = []
    rf_importances = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # XGBoost 모델 학습
        xgboost_model = xgb.XGBRegressor()
        xgboost_model.fit(X_train, y_train)
        xgboost_pred = xgboost_model.predict(X_test)
        xgboost_importances.append(xgboost_model.feature_importances_)

        # RandomForest 모델 학습
        rf_model = RandomForestRegressor(n_estimators=50, random_state=42)
        rf_model.fit(X_train, y_train)
        rf_pred = rf_model.predict(X_test)
        rf_importances.append(rf_model.feature_importances_)

        # 앙상블을 위한 예측값 결합
        y_pred = (xgboost_pred + rf_pred) / 2

        # 평가: MSE 계산
        mse = mean_squared_error(y_test, y_pred)
        mse_list.append(mse)

    # 평균 MSE 계산
    avg_mse = sum(mse_list) / len(mse_list)
    mse_1_dict[label] = avg_mse
    print("Mean Squared Error for", label, ":", avg_mse)

    # 평균 feature importance 계산
    avg_xgboost_importances = np.mean(xgboost_importances, axis=0)
    avg_rf_importances = np.mean(rf_importances, axis=0)

    # Combined average feature importance
    combined_importances = (avg_xgboost_importances + avg_rf_importances) / 2
    feature_importance_dict[label] = combined_importances

    # 상위 50개 feature 선택
    feature_names = X.columns
    top_50_indices = np.argsort(combined_importances)[-50:][::-1]
    top_50_features = feature_names[top_50_indices]

    top_50_features_dict[label] = top_50_features

# Display top 50 features for each label
for label in target:
    print(f"Top 50 features for {label}:")
    print(top_50_features_dict[label])